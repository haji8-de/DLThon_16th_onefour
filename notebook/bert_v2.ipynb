{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9d5a913",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/torch_gpu/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import f1_score, confusion_matrix, roc_curve, auc\n",
    "\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizerFast\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, normalizers\n",
    "from konlpy.tag import Okt, Kkma, Mecab\n",
    "import sentencepiece as spm\n",
    "\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59c185e",
   "metadata": {},
   "source": [
    "### config 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83c4fb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {'협박 대화': 0, '갈취 대화': 1, '직장 내 괴롭힘 대화': 2, '기타 괴롭힘 대화': 3, '일반 대화': 4}\n",
    "class_names = ['협박 대화', '갈취 대화', '직장괴롭힘 대화', '기타괴롭힘 대화', '일반 대화']\n",
    "class_map = {0: '협박 대화', 1: '갈취 대화', 2: '직장 내 괴롭힘 대화', 3: '기타 괴롭힘 대화', 4: '일반 대화'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4fec9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a04f469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def font_setting():\n",
    "    plt.rc('font', family='NanumGothic')\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    sns.set_theme(style=\"whitegrid\", font='NanumGothic', palette=\"muted\")\n",
    "\n",
    "font_setting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b190593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(module):\n",
    "    \"\"\"BERT 표준 가중치 초기화 (std=0.01)\"\"\"\n",
    "    if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "        module.weight.data.normal_(mean=0.0, std=0.01)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "    elif isinstance(module, nn.LayerNorm):\n",
    "        module.bias.data.zero_()\n",
    "        module.weight.data.fill_(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528d47e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertConfig:\n",
    "    def __init__(self):\n",
    "        # 모델 구조\n",
    "        self.vocab_size = 35000\n",
    "        self.d_model = 256\n",
    "        self.num_layers = 4\n",
    "        self.num_heads = 8\n",
    "        self.ff_dim = 1024\n",
    "        self.max_pos = 128  # 문장의 최대 길이\n",
    "        self.dropout = 0.3\n",
    "        \n",
    "        #모델 경로\n",
    "        self.model_path = None\n",
    "\n",
    "        # 학습 설정\n",
    "        self.max_seq_len = 128 # 토큰 개수 기준\n",
    "        self.batch_size = 16\n",
    "        self.learning_rate = 5e-5\n",
    "        self.weight_decay = 0.1\n",
    "        self.epochs = 100\n",
    "        self.patience = 20\n",
    "        \n",
    "        # 특수 토큰 ID\n",
    "        self.force_train = True             # \"wordpiece\", \"spm\" 재학습 시\n",
    "        self.tokenizer_type = 'pretrained'   # \"wordpiece\", \"pretrained\", \"spm\", 'okt', 'kkma', 'mecab'\n",
    "        self.spm_model_type = 'unigram'      # 'bpe', 'unigram'\n",
    "        self.pretrained_model_name = \"klue/bert-base\"  # 한국어 전용 사전학습 모델 예시\n",
    "        self.pad_id = 0\n",
    "        self.unk_id = 1\n",
    "        self.cls_id = 2\n",
    "        self.sep_id = 3\n",
    "        self.mask_id = 4\n",
    "\n",
    "        self.pad_token = '[PAD]'\n",
    "        self.unk_token = '[UNK]'\n",
    "        self.cls_token = '[CLS]'\n",
    "        self.sep_token = '[SEP]'\n",
    "        self.mask_token = '[MASK]'\n",
    "\n",
    "    def update_from_tokenizer(self, tokenizer):\n",
    "        \"\"\"\n",
    "        최종적으로 토크나이저의 상태를 config에 동기화합니다.\n",
    "        \"\"\"\n",
    "        # 1. 실제 사용 중인 토크나이저의 어휘 사전 크기로 업데이트\n",
    "        self.vocab_size = len(tokenizer)\n",
    "\n",
    "        # 2. [보완] SPTokenizer인 경우, 실제 할당된 mask_id를 가져오는 로직 추가\n",
    "        # 만약 tokenizer에 word2idx가 있다면 직접 조회하는 것이 가장 정확합니다.\n",
    "        if hasattr(tokenizer, 'word2idx'):\n",
    "            self.mask_id = tokenizer.word2idx.get('[MASK]', self.mask_id)\n",
    "\n",
    "        # 3. ID 중복 여부 체크 (더 포괄적으로 변경)\n",
    "        ids = [self.pad_id, self.unk_id, self.cls_id, self.sep_id]\n",
    "        if len(ids) != len(set(ids)):\n",
    "            print(f\">> [Warning] 특수 토큰 ID 간에 중복이 감지되었습니다! 현재 상태: {ids}\")\n",
    "            # 필요 시 여기서 강제 재조정 로직 수행\n",
    "\n",
    "        # 4. 최종 확정된 ID 상태 출력\n",
    "        print(f\"[Config Finalized] Vocab: {self.vocab_size}\")\n",
    "        # mask_token 대신 mask_id를 출력하여 숫자값을 확인하세요.\n",
    "        print(f\" >> IDs - PAD:{self.pad_id}, UNK:{self.unk_id}, CLS:{self.cls_id}, SEP:{self.sep_id}, MASK:{self.mask_id}\")\n",
    "    \n",
    "    def save(self, path):\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(self.__dict__, f, indent=4)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path):\n",
    "        with open(path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        config = BertConfig()\n",
    "        config.__dict__.update(data)\n",
    "        return config\n",
    "    \n",
    "config = BertConfig()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68923f15",
   "metadata": {},
   "source": [
    "---\n",
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3340fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_tagging(config):\n",
    "    if config.tokenizer_type == \"pretrained\":\n",
    "        model_tag = config.pretrained_model_name.replace(\"/\", \"_\")\n",
    "    elif config.tokenizer_type == \"spm\":\n",
    "        model_tag = f\"spm_{config.spm_model_type}\"\n",
    "    elif config.tokenizer_type == \"wordpiece\":\n",
    "        model_tag = \"wordpiece\" # 혹은 \"wp\"\n",
    "    else:\n",
    "        model_tag = config.tokenizer_type\n",
    "    return model_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa5847b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_training_plots(history, model_tag, all_labels, logits, num_labels=5):\n",
    "    \"\"\"\n",
    "    학습 결과(Loss/Acc)와 클래스별 ROC 커브를 시각화하여 저장합니다.\n",
    "    \"\"\"\n",
    "    font_setting()\n",
    "    # 0. 클래스 맵 설정 (범례 표시용)\n",
    "    class_names = ['Threat', 'Extortion', 'Workplace', 'Other', 'Normal']\n",
    "    save_dir = f\"./results/{model_tag}\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # 1. Train vs Val (Loss & Acc) 그리기\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    plt.style.use('seaborn-v0_8-muted') # 깔끔한 테마 적용\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Loss Plot\n",
    "    ax1.plot(epochs, history['train_loss'], 'b-o', label='Train Loss', markersize=4)\n",
    "    ax1.plot(epochs, history['val_loss'], 'r-o', label='Val Loss', markersize=4)\n",
    "    ax1.set_title('Learning Curve: Loss', fontsize=13, fontweight='bold')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.grid(True, linestyle='--', alpha=0.6)\n",
    "    ax1.legend()\n",
    "\n",
    "    # Accuracy Plot\n",
    "    ax2.plot(epochs, history['train_acc'], 'b-o', label='Train Acc', markersize=4)\n",
    "    ax2.plot(epochs, history['val_acc'], 'r-o', label='Val Acc', markersize=4)\n",
    "    ax2.set_title('Learning Curve: Accuracy', fontsize=13, fontweight='bold')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.grid(True, linestyle='--', alpha=0.6)\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_dir}/learning_curves.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # 2. ROC Curve 계산 및 그리기 (Multi-class OvR)\n",
    "    # [수정] logits가 리스트 형태의 텐서들일 경우를 대비해 안전하게 병합\n",
    "    all_logits_tensor = torch.tensor(np.array(logits)).float()\n",
    "\n",
    "    # 마지막 차원(dim=-1)을 기준으로 소프트맥스 적용\n",
    "    y_score = torch.softmax(all_logits_tensor, dim=-1).numpy()\n",
    "    \n",
    "    # 정답 레이블 이진화 (0 -> [1,0,0,0,0])\n",
    "    y_test = label_binarize(all_labels, classes=list(range(num_labels)))\n",
    "    \n",
    "    plt.figure(figsize=(9, 7))\n",
    "    \n",
    "    # 각 클래스별로 ROC 계산\n",
    "    for i in range(num_labels):\n",
    "        fpr, tpr, _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        # 실제 클래스 이름을 사용하여 그래프 그림\n",
    "        label_name = class_names[i] if i < len(class_names) else f'Class {i}'\n",
    "        plt.plot(fpr, tpr, lw=2, label=f'{label_name} (AUC = {roc_auc:.3f})')\n",
    "        \n",
    "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--', lw=1) # 기준선\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "    plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "    plt.title(f'Multi-class ROC Curve ({model_tag})', fontsize=14, fontweight='bold')\n",
    "    plt.legend(loc='lower right', fontsize=10)\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.savefig(f\"{save_dir}/roc_curve.png\", dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\">> [Success] 시각화 결과가 저장되었습니다: {save_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2630a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_save_confusion_matrix(y_true, y_pred, class_names, save_path):\n",
    "    font_setting()\n",
    "    # 1. 혼동 행렬 계산\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # 2. 그래프 설정\n",
    "    plt.figure(figsize=(10, 8)) # 그림 크기 설정\n",
    "    \n",
    "    # Seaborn 히트맵 그리기\n",
    "    # annot=True: 셀 안에 숫자 표시, fmt='d': 정수형 포맷, cmap: 색상 테마\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    \n",
    "    # 축 라벨 및 제목 설정\n",
    "    plt.xlabel('Predicted Label') # 모델이 예측한 라벨\n",
    "    plt.ylabel('True Label')      # 실제 정답 라벨\n",
    "    plt.title('Confusion Matrix (Validation Set)')\n",
    "    plt.xticks(rotation=45) # X축 라벨이 길 경우를 대비해 회전\n",
    "\n",
    "    # 3. 이미지 저장 및 닫기\n",
    "    plt.tight_layout() # 여백 자동 조정\n",
    "    plt.savefig(save_path, dpi=300) # 고해상도 저장\n",
    "    plt.close() # 메모리 해제를 위해 플롯 닫기\n",
    "    print(f\">> Confusion Matrix image saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01abd85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug(train_loader, tokenizer):\n",
    "     # 1. DataLoader에서 배치 하나 가져오기\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    ids, labels = sample_batch\n",
    "\n",
    "    # 2. 첫 번째 데이터 샘플링\n",
    "    first_sample = ids[0].tolist()\n",
    "\n",
    "    # 3. 육안 검증 (숫자 확인)\n",
    "    print(\"--- [1] 숫자 ID 검증 ---\")\n",
    "    print(f\"맨 앞 (CLS 예상): {first_sample[0]} (2이면 성공)\")\n",
    "    # 0(PAD)이 나오기 직전 값이 3(SEP)인지 확인\n",
    "    last_token_idx = (ids[0] != 0).sum().item() - 1 \n",
    "    print(f\"문장 끝 (SEP 예상): {first_sample[last_token_idx]} (3이면 성공)\")\n",
    "    print(f\"패딩 시작 (PAD 예상): {first_sample[last_token_idx + 1] if last_token_idx + 1 < 128 else 'N/A'} (0이면 성공)\")\n",
    "\n",
    "    # 4. 문자열 검증 (토크나이저 복원)\n",
    "    print(\"\\n--- [2] 토크나이저 디코드 검증 ---\")\n",
    "    decoded = tokenizer.decode(first_sample)\n",
    "    print(f\"복원된 문장: {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "799d1f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, config, epoch, f1, loss, path):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'config': config.__dict__,\n",
    "        'val_f1': f1, \n",
    "        'val_loss': loss\n",
    "    }, path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a56241",
   "metadata": {},
   "source": [
    "---\n",
    "### 모델 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc5001e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 추론용 데이터셋\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, config):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.config = config\n",
    "\n",
    "    def __len__(self): return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        tokens = self.tokenizer.encode(text)\n",
    "        encoded = [self.config.cls_id] + tokens + [self.config.sep_id]\n",
    "        \n",
    "        if len(encoded) < self.config.max_seq_len:\n",
    "            encoded += [self.config.pad_id] * (self.config.max_seq_len - len(encoded))\n",
    "        else:\n",
    "            encoded = encoded[:self.config.max_seq_len]\n",
    "        return torch.tensor(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b50d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardBertModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # 1. 임베딩 층 (Token + Position + Segment)\n",
    "        self.token_emb = nn.Embedding(config.vocab_size, config.d_model)\n",
    "        self.pos_emb = nn.Embedding(config.max_pos, config.d_model)\n",
    "        self.seg_emb = nn.Embedding(2, config.d_model)\n",
    "        \n",
    "        self.norm = nn.LayerNorm(config.d_model, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "        # 2. 트랜스포머 인코더 블록들\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.ModuleDict({\n",
    "                # MHA: Scaled Dot-Product Attention 기반 병렬 헤드\n",
    "                'mha': nn.MultiheadAttention(config.d_model, config.num_heads, \n",
    "                                             dropout=config.dropout, batch_first=True),\n",
    "                'norm1': nn.LayerNorm(config.d_model, eps=1e-12),\n",
    "                # FFN: Position-wise Feed Forward Network\n",
    "                'ffn': nn.Sequential(\n",
    "                    nn.Linear(config.d_model, config.ff_dim),\n",
    "                    nn.GELU(),\n",
    "                    nn.Dropout(config.dropout),\n",
    "                    nn.Linear(config.ff_dim, config.d_model),\n",
    "                    nn.Dropout(config.dropout)\n",
    "                ),\n",
    "                'norm2': nn.LayerNorm(config.d_model, eps=1e-12)\n",
    "            }) for _ in range(config.num_layers)\n",
    "        ])\n",
    "        \n",
    "        # 3. Pooler: 토큰의 출력을 압축하여 문장 벡터 생성\n",
    "        self.pooler = nn.Sequential(\n",
    "            nn.Linear(config.d_model, config.d_model),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, mask=None):\n",
    "        batch_size, seq_len = input_ids.size()\n",
    "        \n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "        \n",
    "        # 위치 ID 생성 및 배치 크기에 맞게 확장\n",
    "        pos_ids = torch.arange(seq_len, device=input_ids.device).unsqueeze(0)\n",
    "        pos_ids = pos_ids.expand(batch_size, seq_len)\n",
    "        \n",
    "        # 임베딩 합산: 핵심 문맥 정보 결합\n",
    "        x = self.token_emb(input_ids) + self.pos_emb(pos_ids) + self.seg_emb(token_type_ids)\n",
    "        x = self.norm(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # MHA용 패딩 마스크 생성 (0인 곳을 True로 변환하여 연산 제외)\n",
    "        padding_mask = (mask == 0) if mask is not None else None\n",
    "\n",
    "        # 인코더 레이어 순회\n",
    "        for layer in self.layers:\n",
    "            # 1) Multi-Head Attention + Residual + LayerNorm\n",
    "            attn_out, _ = layer['mha'](x, x, x, key_padding_mask=padding_mask)\n",
    "            x = layer['norm1'](x + attn_out)\n",
    "            \n",
    "            # 2) Feed Forward Network + Residual + LayerNorm\n",
    "            ffn_out = layer['ffn'](x)\n",
    "            x = layer['norm2'](x + ffn_out)\n",
    "            \n",
    "        # 전체 시퀀스 출력 x와 [CLS] 토큰 기반의 문장 벡터 반환\n",
    "        return x, self.pooler(x[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c931b4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForSequenceClassification(nn.Module):\n",
    "    def __init__(self, backbone, config, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = backbone\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        # 최종 분류기: 문장 벡터를 입력받아 클래스 개수만큼 출력\n",
    "        self.classifier = nn.Linear(config.d_model, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, mask=None):\n",
    "        # backbone에서 pooler 출력을 가져옴\n",
    "        _, pooled_output = self.bert(input_ids, token_type_ids, mask)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5093380f",
   "metadata": {},
   "source": [
    "---\n",
    "### Data Management Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96bcb0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = re.sub(r'[^가-힣]', ' ', text)\n",
    "    return re.sub(r'\\s+', ' ', text).strip()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03adde24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, config, is_train=False, mask_prob=0.25):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.config = config\n",
    "        self.labels = labels\n",
    "        self.is_train = is_train  # 훈련 데이터셋일 때만 마스킹 적용\n",
    "        self.mask_prob = mask_prob # 마스킹할 확률 (기본 25%)\n",
    "        self.data = []\n",
    "\n",
    "        for t in texts:\n",
    "            tokens = self.tokenizer.encode(t)\n",
    "            tokens = tokens[:config.max_seq_len - 2]\n",
    "            formatted_tokens = [config.cls_id] + tokens + [config.sep_id]\n",
    "            self.data.append(torch.tensor(formatted_tokens, dtype=torch.long))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "            tokens = self.data[idx].clone()\n",
    "            label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "            \n",
    "            # target_len은 조건문 상관없이 항상 필요하므로 맨 위나 밖에서 정의합니다.\n",
    "            target_len = self.config.max_seq_len \n",
    "\n",
    "            # 2. 훈련 시에만 Input Masking 적용\n",
    "            if self.is_train:\n",
    "                actual_len = len(tokens)\n",
    "                # CLS(0) ~ SEP(actual_len-1) 사이만 마스킹\n",
    "                for i in range(1, actual_len - 1): \n",
    "                    \n",
    "                    # 25% 확률로 마스킹 대상 선정\n",
    "                    if random.random() < self.mask_prob: \n",
    "                        # 80-10-10 룰 적용\n",
    "                        dice = random.random()\n",
    "                        \n",
    "                        if dice < 0.8:\n",
    "                            # 80%: [MASK] 토큰으로 교체\n",
    "                            tokens[i] = getattr(self.config, 'mask_id', 4)\n",
    "                        elif dice < 0.9:\n",
    "                            # 10%: 랜덤 단어로 교체\n",
    "                            tokens[i] = random.randint(0,  self.config.vocab_size - 1)\n",
    "                        else:\n",
    "                            # 10%: 원래 단어 유지\n",
    "                            pass\n",
    "            \n",
    "            # 너무 길면 자르기 (Truncation)\n",
    "            if len(tokens) > target_len:\n",
    "                tokens = tokens[:target_len]\n",
    "                tokens[-1] = self.config.sep_id \n",
    "\n",
    "            # 너무 짧으면 늘리기 (Padding)\n",
    "            elif len(tokens) < target_len:\n",
    "                pad_len = target_len - len(tokens)\n",
    "                pad_tensor = torch.full((pad_len,), self.config.pad_id, dtype=torch.long)\n",
    "                tokens = torch.cat([tokens, pad_tensor])\n",
    "            \n",
    "            return tokens, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c1d9be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_dataset(df, class_column='class'):\n",
    "    \"\"\"명시적 반복문을 사용하여 데이터를 클래스별로 균등하게 샘플링합니다.\"\"\"\n",
    "    # 1. 각 클래스별 최소 개수 확인\n",
    "    min_count = df[class_column].value_counts().min()\n",
    "    print(f\">> [Data Balancing] 최소 데이터 클래스 개수 기준: {min_count}개\")\n",
    "\n",
    "    # 2. 명시적 반복문을 사용하여 샘플링\n",
    "    balanced_dfs = []\n",
    "    for label, group in df.groupby(class_column):\n",
    "        # 각 클래스 그룹에서 동일하게 min_count만큼 샘플링\n",
    "        sampled_group = group.sample(min_count, random_state=42)\n",
    "        balanced_dfs.append(sampled_group)\n",
    "\n",
    "    # 3. 데이터 합치기 및 셔플\n",
    "    df_balanced = pd.concat(balanced_dfs)\n",
    "    df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    print(f\">> 밸런싱 완료! 전체 데이터 개수: {len(df_balanced)}\")\n",
    "    print(f\">> 클래스 분포:\\n{df_balanced[class_column].value_counts()}\")\n",
    "    \n",
    "    return df_balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66045498",
   "metadata": {},
   "source": [
    "---\n",
    "### tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4f2b39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 학습 모델의 토크나이저\n",
    "class PretrainedHFTokenizer:\n",
    "    def __init__(self, model_name, config):\n",
    "        print(f\"사전학습된 토크나이저 로드 중: {model_name}\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.word2idx = self.tokenizer.get_vocab()\n",
    "        \n",
    "        # [수정] 사전학습 모델의 ID를 config에 정확히 동기화\n",
    "        config.pad_id = self.tokenizer.pad_token_id\n",
    "        config.unk_id = self.tokenizer.unk_token_id\n",
    "        config.cls_id = self.tokenizer.cls_token_id\n",
    "        config.sep_id = self.tokenizer.sep_token_id\n",
    "        \n",
    "        # [수정] mask_id도 ID로 가져와서 저장 (없으면 None)\n",
    "        mask_token = self.tokenizer.mask_token or \"[MASK]\"\n",
    "        config.mask_id = self.tokenizer.convert_tokens_to_ids(mask_token)\n",
    "\n",
    "    def encode(self, text):\n",
    "        return self.tokenizer.encode(text, add_special_tokens=False)\n",
    "\n",
    "    def decode(self, ids, skip_special_tokens=False):\n",
    "        return self.tokenizer.decode(ids, skip_special_tokens=skip_special_tokens)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return self.tokenizer.tokenize(text)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be79ec55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordPiece\n",
    "class WordPieceTokenizer:\n",
    "    def __init__(self, corpus_list, vocab_size, config, model_prefix=None, force_train=False):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.model_prefix = model_prefix or f\"./wordpiece/wp_v{self.vocab_size}\"\n",
    "        self.json_path = f\"{self.model_prefix}.json\"\n",
    "        self.corpus_file = './wordpiece/corpus.txt'\n",
    "\n",
    "        os.makedirs(os.path.dirname(self.json_path), exist_ok=True)\n",
    "\n",
    "        # force_train이 True이거나 파일이 없을 때만 학습하도록 논리 수정\n",
    "        if force_train or not os.path.exists(self.json_path):\n",
    "            print(f\">> [Info] WordPiece 학습을 시작함: {self.json_path}\")\n",
    "            \n",
    "            # [수정] NoneType 에러 방지를 위한 데이터 검증\n",
    "            if corpus_list is None:\n",
    "                raise ValueError(\"학습을 위한 corpus_list가 None임. 데이터를 확인해야 함.\")\n",
    "            \n",
    "            # 학습용 말뭉치 파일 생성\n",
    "            with open(self.corpus_file, 'w', encoding='utf-8') as f:\n",
    "                for text in corpus_list:\n",
    "                    if text is not None:  # 요소가 None인 경우 방지\n",
    "                        f.write(str(text).strip() + \"\\n\")\n",
    "\n",
    "            # WordPiece 모델 설정\n",
    "            tokenizer = Tokenizer(models.WordPiece(unk_token=config.unk_token))\n",
    "            tokenizer.normalizer = normalizers.BertNormalizer() \n",
    "            tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()\n",
    "            \n",
    "            special_tokens = [config.pad_token, config.unk_token, config.cls_token, config.sep_token, config.mask_token]\n",
    "            trainer = trainers.WordPieceTrainer(vocab_size=self.vocab_size, special_tokens=special_tokens)\n",
    "            \n",
    "            # 학습 진행\n",
    "            tokenizer.train(files=[self.corpus_file], trainer=trainer)\n",
    "            \n",
    "            # 모델 전체를 JSON으로 저장\n",
    "            tokenizer.save(self.json_path)\n",
    "            print(f\">> [Info] WordPiece 학습 및 JSON 저장 완료: {self.json_path}\")\n",
    "        else:\n",
    "            print(f\">> [Info] 기존에 학습된 모델을 로드함: {self.json_path}\")\n",
    "\n",
    "        # 2. 인터페이스 로드 (JSON 파일을 직접 읽음)\n",
    "        self.tokenizer = PreTrainedTokenizerFast(\n",
    "            tokenizer_file=self.json_path,\n",
    "            pad_token=config.pad_token,\n",
    "            unk_token=config.unk_token,\n",
    "            cls_token=config.cls_token,\n",
    "            sep_token=config.sep_token,\n",
    "            mask_token=config.mask_token\n",
    "        )\n",
    "        self.word2idx = self.tokenizer.get_vocab()\n",
    "\n",
    "    def encode(self, text): return self.tokenizer.encode(text, add_special_tokens=False)\n",
    "    def tokenize(self, text): return self.tokenizer.tokenize(text)\n",
    "    def decode(self, ids): return self.tokenizer.decode(ids)\n",
    "    def __len__(self): return len(self.tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb16d290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SentencePiece 토크나이저 ---\n",
    "class SPTokenizer:\n",
    "    def __init__(self, corpus_list, vocab_size, config, model_prefix=None, force_train=False):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.corpus_file = './spm/corpus.txt'\n",
    "\n",
    "        model_tag = f\"spm_{config.spm_model_type}_v{self.vocab_size}\"\n",
    "        \n",
    "        # 2. 경로 설정: 외부에서 지정하지 않으면 태그를 기반으로 기본 경로 생성\n",
    "        if model_prefix is None:\n",
    "            self.model_prefix = f'./spm/{model_tag}'\n",
    "        else:\n",
    "            self.model_prefix = model_prefix\n",
    "            \n",
    "        model_path = f'{self.model_prefix}.model'\n",
    "        \n",
    "        # 폴더 생성\n",
    "        os.makedirs(os.path.dirname(self.corpus_file), exist_ok=True)\n",
    "        os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "        \n",
    "        need_train = force_train or not os.path.exists(model_path)\n",
    "\n",
    "        if not need_train:\n",
    "            temp_sp = spm.SentencePieceProcessor()\n",
    "            temp_sp.load(model_path)\n",
    "            if temp_sp.get_piece_size() != self.vocab_size:\n",
    "                need_train = True\n",
    "\n",
    "        if need_train:\n",
    "            with open(self.corpus_file, 'w', encoding='utf-8') as f:\n",
    "                for text in corpus_list:\n",
    "                    f.write(text + \"\\n\")\n",
    "            \n",
    "            # mask_id 파라미터 제거 및 user_defined_symbols 리스트화\n",
    "            spm.SentencePieceTrainer.Train(\n",
    "                input=self.corpus_file,\n",
    "                model_prefix=self.model_prefix,\n",
    "                vocab_size=self.vocab_size,\n",
    "                character_coverage=0.9995,\n",
    "                model_type=config.spm_model_type,\n",
    "                pad_id=config.pad_id,\n",
    "                unk_id=config.unk_id,\n",
    "                bos_id=config.cls_id,\n",
    "                eos_id=config.sep_id,\n",
    "                pad_piece='[PAD]',\n",
    "                unk_piece='[UNK]',\n",
    "                bos_piece='[CLS]',\n",
    "                eos_piece='[SEP]',\n",
    "                user_defined_symbols=['[MASK]'] # 리스트로 전달\n",
    "            )\n",
    "            \n",
    "        self.sp = spm.SentencePieceProcessor()\n",
    "        self.sp.load(model_path)\n",
    "        self.word2idx = {self.sp.id_to_piece(i): i for i in range(self.sp.get_piece_size())}\n",
    "\n",
    "    def encode(self, text): return self.sp.encode_as_ids(text)\n",
    "    def tokenize(self, text): return self.sp.encode_as_pieces(text)\n",
    "    def decode(self, ids): return self.sp.decode_ids(ids)\n",
    "    def __len__(self): return self.sp.get_piece_size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9852c2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2.KoNLPy 공통 구조 (속성명 통일) ---\n",
    "class KoNLPyTokenizer:\n",
    "    def __init__(self, tagger_type=\"okt\", corpus=None, vocab_size=5000, force_train=False):\n",
    "        self.vocab_path = f\"{tagger_type}_vocab.json\"\n",
    "        self.tagger_type = tagger_type\n",
    "        self.vocab_size = vocab_size\n",
    "        self.special_tokens = ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]']\n",
    "        \n",
    "        # 1. 태거 초기화\n",
    "        self._init_tagger(tagger_type)\n",
    "\n",
    "        os.makedirs(os.path.dirname(self.vocab_path), exist_ok=True)\n",
    "\n",
    "        # 상황 1: 강제 학습 모드이거나, 저장된 사전 파일이 없는 경우 -> '새로 만들기'\n",
    "        if force_train or not os.path.exists(self.vocab_path):\n",
    "            if corpus is not None:\n",
    "                print(f\"사전 생성을 시작합니다. (force_train={force_train})\")\n",
    "                self._build_and_save_vocab(corpus)\n",
    "            else:\n",
    "                # 학습할 데이터(corpus)도 없고 파일도 없는 경우의 예외 처리\n",
    "                print(\"경고: 저장된 사전 파일이 없고 학습할 데이터(corpus)도 없습니다. 기본 토큰으로 초기화합니다.\")\n",
    "                self.word2idx = {t: i for i, t in enumerate(self.special_tokens)}\n",
    "                self.idx2word = {i: t for i, t in enumerate(self.special_tokens)}\n",
    "\n",
    "        # 상황 2: 이미 파일이 존재하고, 새로 학습할 필요가 없는 경우 -> '불러오기'\n",
    "        else:\n",
    "            print(f\"기존 사전을 로드합니다: {self.vocab_path}\")\n",
    "            self._load_vocab()\n",
    "    \n",
    "    def _init_tagger(self, tagger_type):\n",
    "        if tagger_type == \"okt\": self.tagger = Okt()\n",
    "        elif tagger_type == \"kkma\": self.tagger = Kkma()\n",
    "        elif tagger_type == \"mecab\": self.tagger = Mecab()\n",
    "        else: raise ValueError(\"지원하지 않는 태거 타입입니다.\")\n",
    "\n",
    "    def _build_and_save_vocab(self, corpus):\n",
    "        \"\"\"말뭉치로부터 사전을 만들고 파일로 저장합니다.\"\"\"\n",
    "        print(f\"새로운 사전을 생성하여 '{self.vocab_path}'에 저장합니다...\")\n",
    "        \n",
    "        # 토큰화 및 빈도 계산\n",
    "        all_tokens = [t for text in corpus for t in (self.tagger.morphs(text) or [])]\n",
    "        counts = Counter(all_tokens)\n",
    "        \n",
    "        # 사전 구성\n",
    "        vocab = self.special_tokens + [w for w, _ in counts.most_common(self.vocab_size - len(self.special_tokens))]\n",
    "        self.word2idx = {word: i for i, word in enumerate(vocab)}\n",
    "        self.idx2word = {i: word for word, i in self.word2idx.items()}\n",
    "\n",
    "        # 파일 저장\n",
    "        data = {\n",
    "            'tagger_type': self.tagger_type,\n",
    "            'word2idx': self.word2idx\n",
    "        }\n",
    "        with open(self.vocab_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "        print(\"사전 생성 및 저장 완료.\")\n",
    "\n",
    "    def _load_vocab(self):\n",
    "        \"\"\"저장된 파일을 읽어와서 객체에 적용합니다.\"\"\"\n",
    "        print(f\"'{self.vocab_path}'에서 사전을 불러오는 중...\")\n",
    "        with open(self.vocab_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        if self.tagger_type != data['tagger_type']:\n",
    "            print(f\"주의: 설정된 태거가 저장된 태거와 다릅니다. 저장된 설정을 따릅니다.\")\n",
    "            self._init_tagger(data['tagger_type'])\n",
    "            \n",
    "        self.word2idx = data['word2idx']\n",
    "        self.idx2word = {int(index): word for word, index in self.word2idx.items()}\n",
    "        \n",
    "        print(f\"사전 로드 완료. (단어 수: {len(self.word2idx)})\")\n",
    "\n",
    "    def encode(self, text):\n",
    "        return [self.word2idx.get(t, self.word2idx['[UNK]']) for t in self.tagger.morphs(text)]\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        return \" \".join([self.idx2word.get(int(i), '[UNK]') for i in ids])\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"텍스트를 인덱싱하지 않고 형태소 단위로만 분리합니다.\"\"\"\n",
    "        return self.tagger.morphs(text)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66f8589e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(config, corpus=None):\n",
    "    force_train = getattr(config, 'force_train', False)\n",
    "    \n",
    "    # 기본 ID 설정\n",
    "    if config.tokenizer_type != \"pretrained\":\n",
    "        config.pad_id, config.unk_id, config.cls_id, config.sep_id, config.mask_id = 0, 1, 2, 3, 4\n",
    "        config.mask_token = '[MASK]'\n",
    "\n",
    "    if config.tokenizer_type == \"pretrained\":\n",
    "        tokenizer = PretrainedHFTokenizer(config.pretrained_model_name, config)\n",
    "    \n",
    "    elif config.tokenizer_type == \"spm\":\n",
    "        tokenizer = SPTokenizer(corpus, config.vocab_size, config, force_train=force_train)\n",
    "        \n",
    "    # [추가] WordPiece 분기\n",
    "    elif config.tokenizer_type == \"wordpiece\":\n",
    "        tokenizer = WordPieceTokenizer(corpus, config.vocab_size, config, force_train=force_train)\n",
    "    \n",
    "    elif config.tokenizer_type in [\"okt\", \"kkma\", \"mecab\"]:\n",
    "        tokenizer = KoNLPyTokenizer(config.tokenizer_type, corpus, config.vocab_size, force_train=force_train)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"지원하지 않는 토크나이저 타입: {config.tokenizer_type}\")\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c115bf",
   "metadata": {},
   "source": [
    "### Train Validation 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f7dba2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_with_warmup_lr_lambda(total_steps, warmup_steps, min_lr_ratio=1e-7):\n",
    "    def lr_lambda(current_step):\n",
    "        # 1. Linear Warmup 구간\n",
    "        if current_step < warmup_steps:\n",
    "            return float(current_step) / float(max(1, warmup_steps))\n",
    "        \n",
    "        # 2. Cosine Annealing 구간\n",
    "        progress = float(current_step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "        \n",
    "        # cosine_decay는 1.0 ~ min_lr_ratio로 변함\n",
    "        cosine_decay = 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "        \n",
    "        # (1.0 - min_lr_ratio) 범위를 곱해주고 마지막에 min_lr_ratio를 더함\n",
    "        return min_lr_ratio + (1.0 - min_lr_ratio) * cosine_decay\n",
    "        \n",
    "    return lr_lambda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "798cddae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics_classification(logits, labels):\n",
    "    \"\"\"분류 정확도 계산\"\"\"\n",
    "    preds = torch.argmax(logits, dim=-1)\n",
    "    correct = (preds == labels).sum().item()\n",
    "    total = labels.size(0)\n",
    "    return correct, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46dce466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 검증 함수\n",
    "def validate(model, dataloader, criterion, device, config):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_logits = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for ids, labels in dataloader:\n",
    "            ids, labels = ids.to(device), labels.to(device)\n",
    "            \n",
    "            # [핵심 수정] 패딩 마스크 생성 (0이 아닌 곳만 1)\n",
    "            mask = (ids != config.pad_id).long()\n",
    "            \n",
    "            # [핵심 수정] 모델 호출 시 mask 인자 전달\n",
    "            # 반환값 구조에 따라 필요시 _ 처리 (예: _, logits = model(...))\n",
    "            logits = model(ids, mask=mask)\n",
    "            \n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_logits.extend(logits.cpu().numpy())\n",
    "            \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    # Macro F1: 클래스 불균형이 있을 때 소수 클래스 성능을 잘 반영함\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    \n",
    "    # 정확도 계산\n",
    "    acc = (np.array(all_preds) == np.array(all_labels)).mean()\n",
    "    \n",
    "    # 6개의 값을 반환하여 메트릭 확인과 시각화를 동시에 지원\n",
    "    return avg_loss, acc, f1, all_labels, all_preds, all_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b8aa651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Epoch 학습 함수\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, scheduler, device, config):\n",
    "    model.train()\n",
    "    total_loss, total_correct, total_total = 0, 0, 0\n",
    "\n",
    "    for ids, labels in dataloader:\n",
    "        ids, labels = ids.to(device), labels.to(device)\n",
    "        \n",
    "        # [추가] 패딩 마스크 생성: 실제 토큰은 1, 패딩은 0\n",
    "        # (Accuracy:높음 - ids != pad_id 로직)\n",
    "        mask = (ids != config.pad_id).long()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # [수정] 모델 호출 시 mask 인자 반드시 전달\n",
    "        logits = model(ids, mask=mask) \n",
    "        \n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # 그래디언트 클리핑: max_norm=1.0 (Accuracy:높음 - BERT 표준)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # 스케줄러 업데이트 (배치 단위)\n",
    "        scheduler.step()\n",
    "        \n",
    "        # 통계 계산\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # 정확도 지표 계산 (정답 수 c, 전체 샘플 수 t)\n",
    "        c, t = calculate_metrics_classification(logits, labels)\n",
    "        total_correct += c\n",
    "        total_total += t\n",
    "        \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_acc = total_correct / total_total if total_total > 0 else 0\n",
    "    \n",
    "    return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f450ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config, model, train_loader, val_loader):\n",
    "    model_tag = model_tagging(config)\n",
    "    config.model_path = f\"./models/best_bert_f1_{model_tag}.pth\"\n",
    "    os.makedirs(\"./models\", exist_ok=True)\n",
    "\n",
    "    optimizer = optimizer = optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "    TOTAL_STEPS = len(train_loader) * config.epochs\n",
    "    scheduler = lr_scheduler.LambdaLR(\n",
    "        optimizer, lr_lambda=get_cosine_with_warmup_lr_lambda(TOTAL_STEPS, int(TOTAL_STEPS * 0.05))\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "    early_stop_counter, best_f1 = 0, 0.0\n",
    "    best_val_loss = float('inf')\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "\n",
    "    for epoch in range(config.epochs):\n",
    "        # 모든 지표를 계산하여 반환\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, scheduler, device, config)\n",
    "        val_loss, val_acc, val_f1, all_labels, all_preds, all_logits = validate(model, val_loader, criterion, device, config)\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{config.epochs}\")\n",
    "        print(f\"  [Train] Loss: {train_loss:.4f} | Acc: {train_acc:.4f}\")\n",
    "        print(f\"  [Val]   Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | F1: {val_f1:.4f}\")\n",
    "\n",
    "        improved_f1 = val_f1 > best_f1\n",
    "        improved_loss = val_loss < best_val_loss\n",
    "\n",
    "        if improved_f1 or improved_loss:\n",
    "            early_stop_counter = 0\n",
    "            \n",
    "            if improved_f1 or improved_loss:\n",
    "                if improved_f1:\n",
    "                    best_f1 = val_f1\n",
    "                    print(f\"  >> Best F1 Updated! ({val_f1:.4f})\")\n",
    "\n",
    "                if improved_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    print(f\"  >> Best Loss Updated! ({val_loss:.4f})\")\n",
    "\n",
    "                early_stop_counter = 0\n",
    "                save_checkpoint(model, optimizer, config, epoch, val_f1, val_loss, config.model_path)\n",
    "                print(f\"  >> Best model Updated! ({val_f1:.4f}, {val_loss:.4f})\")\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            print(f\"  >> EarlyStopping counter: {early_stop_counter} out of {config.patience}\")\n",
    "\n",
    "        if early_stop_counter >= config.patience:\n",
    "            print(f\"성능 개선이 없어 {epoch+1} 에포크에서 학습을 조기 종료합니다.\")\n",
    "            break\n",
    "\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    print(\"학습이 완료되었습니다. 최종 지표 분석을 시작합니다.\")\n",
    "\n",
    "    if os.path.exists(config.model_path):\n",
    "        checkpoint = torch.load(config.model_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\">> 최고 성능 모델 복원 완료 (Epoch {checkpoint['epoch']+1})\")\n",
    "\n",
    "    # 검증 데이터셋으로 최종 결과 추출\n",
    "    val_acc, val_acc, val_f1, all_labels, all_preds, all_logits = validate(\n",
    "        model, val_loader, criterion, device, config\n",
    "    )\n",
    "    # print(f\"val_acc:{val_acc}, val_acc:{val_acc}, val_f1:{val_f1}\")\n",
    "    # 오타로 수정\n",
    "    print(f\"val_acc:{val_acc:.4f}, val_loss: {val_loss:.4f}, val_f1:{val_f1:.4f}\")\n",
    "\n",
    "    # 최종 시각화 저장 (딱 한 번, 최고의 성능 기준으로)\n",
    "    results_dir = f\"./results/{model_tag}_d{config.d_model}_f{config.ff_dim}_l{config.num_layers}\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    final_cm_path = os.path.join(results_dir, \"final_best_confusion_matrix.png\")\n",
    "\n",
    "    # 혼동 행렬 및 학습 곡선 저장\n",
    "    plot_and_save_confusion_matrix(all_labels, all_preds, class_names, final_cm_path)\n",
    "    save_training_plots(history, model_tag, all_labels, all_logits)\n",
    "    print(f\">> 모든 최종 결과물이 ./results/{model_tag} 에 저장되었습니다.\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "499b0897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_and_predict(config):\n",
    "    \"\"\"\n",
    "    저장된 최고 성능 모델을 로드하여 테스트 데이터에 대해 추론하고 결과를 저장합니다.\n",
    "    \"\"\"\n",
    "    # 1. 모델 태깅 및 경로 설정\n",
    "    model_tag = model_tagging(config)\n",
    "\n",
    "    if not os.path.exists(config.model_path):\n",
    "        raise FileNotFoundError(f\"모델 파일을 찾을 수 없습니다: {config.model_path}\")\n",
    "\n",
    "    # 2. 체크포인트 로드 및 Config 복원\n",
    "    checkpoint = torch.load(config.model_path , map_location=device)\n",
    "    config_dict = checkpoint.get('config', {})\n",
    "    config.__dict__.update(config_dict)  # 학습 당시 하이퍼파라미터 복원\n",
    "    print(f\"모델 가중치 로드 완료 (Epoch: {checkpoint.get('epoch', 'N/A')}, F1: {checkpoint.get('val_f1', 0.0):.4f})\")\n",
    "\n",
    "    # 3. 모델 초기화 및 Eval 모드 설정\n",
    "    backbone = StandardBertModel(config)\n",
    "    model = BertForSequenceClassification(backbone, config, num_labels=5).to(device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    print(f\"모델 가중치 로드 완료 (Epoch: {checkpoint.get('epoch', 'N/A')}, F1: {checkpoint.get('val_f1', 'N/A'):.4f})\")\n",
    "\n",
    "    # 4. 토크나이저 및 데이터 준비\n",
    "    config.force_train = False\n",
    "    tokenizer = get_tokenizer(config)\n",
    "\n",
    "    with open(\"./dataset/test.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        test_json = json.load(f)\n",
    "\n",
    "    test_ids = list(test_json.keys())\n",
    "    raw_texts = [test_json[tid]['text'] for tid in test_ids]\n",
    "    test_texts = [clean_text(text) for text in raw_texts]\n",
    "\n",
    "    test_dataset = TestDataset(test_texts, tokenizer, config)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # 5. 추론 수행\n",
    "    all_preds, all_probs = [], []\n",
    "    print(f\"추론 시작... (테스트 데이터: {len(test_texts)}개)\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Predicting\"):\n",
    "            # 데이터를 GPU/CPU로 이동\n",
    "            ids = batch.to(device)\n",
    "            \n",
    "            # 패딩 마스크 생성 (BERT 필수 인자)\n",
    "            mask = (ids != config.pad_id).long()\n",
    "            \n",
    "            # 모델 포워딩\n",
    "            logits = model(ids, mask=mask)\n",
    "            \n",
    "            # 확률 변환 및 클래스 결정\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            max_prob, pred = torch.max(probs, dim=-1)\n",
    "            \n",
    "            all_probs.extend(max_prob.cpu().numpy())\n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "\n",
    "    # 6. 결과 정리\n",
    "    results_list = []\n",
    "    for tid, text, p, prob in zip(test_ids, raw_texts, all_preds, all_probs):\n",
    "        submission_label = int(p)\n",
    "        text_label = class_map[submission_label]\n",
    "\n",
    "        cleaned_text = clean_text(text)\n",
    "        tokens = tokenizer.tokenize(cleaned_text)\n",
    "        tokenized_str = \" \".join(tokens)\n",
    "        \n",
    "        results_list.append({\n",
    "            \"idx\": tid,\n",
    "            \"class\": submission_label,\n",
    "            \"text_label\": text_label,\n",
    "            \"confidence\": round(float(prob), 4),\n",
    "            \"tokenized_text\": tokenized_str,\n",
    "            \"conversation\": text,\n",
    "            \"cleaned_text\": cleaned_text\n",
    "        })\n",
    "\n",
    "    df_total = pd.DataFrame(results_list)\n",
    "\n",
    "    # 7. 파일 저장\n",
    "    output_dir = f\"./dataset/{model_tag}_d{config.d_model}_f{config.ff_dim}_l{config.num_layers}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    submission_path = os.path.join(output_dir, f\"submission_{model_tag}_{config.vocab_size}.csv\")\n",
    "    result_detail_path = os.path.join(output_dir, f\"test_result_{model_tag}_{config.vocab_size}.csv\")\n",
    "\n",
    "    df_total[[\"idx\", \"class\"]].to_csv(submission_path, index=False)\n",
    "    df_total.to_csv(result_detail_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"최종 결과 저장 완료: {output_dir}\")\n",
    "    return df_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38e766b",
   "metadata": {},
   "source": [
    "### 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0373e0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> [Data Balancing] 최소 데이터 클래스 개수 기준: 890개\n",
      ">> 밸런싱 완료! 전체 데이터 개수: 4450\n",
      ">> 클래스 분포:\n",
      "class\n",
      "갈취 대화          890\n",
      "협박 대화          890\n",
      "일반 대화          890\n",
      "기타 괴롭힘 대화      890\n",
      "직장 내 괴롭힘 대화    890\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 1. 데이터 로드 및 균형 맞추기\n",
    "df = pd.read_csv(\"./dataset/train_v2.csv\").dropna(subset=[\"conversation\", \"class\"])\n",
    "df = balance_dataset(df, class_column='class')\n",
    "\n",
    "df[\"label\"] = df[\"class\"].map(label_dict)\n",
    "df[\"conversation\"] = df[\"conversation\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db3f512",
   "metadata": {},
   "source": [
    "### Koelectra base v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e90ae3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.tokenizer_type = 'pretrained' \n",
    "config.pretrained_model_name = 'monologg/koelectra-base-v3-discriminator'\n",
    "config.num_layers = 4\n",
    "config.d_model = 256\n",
    "config.ff_dim = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e816e2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사전학습된 토크나이저 로드 중: monologg/koelectra-base-v3-discriminator\n",
      "[Config Finalized] Vocab: 35000\n",
      " >> IDs - PAD:0, UNK:1, CLS:2, SEP:3, MASK:4\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(config, df[\"conversation\"].tolist())\n",
    "config.update_from_tokenizer(tokenizer) # Vocab Size 동기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a68a0ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df[\"label\"])\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    ClassificationDataset(train_df[\"conversation\"].tolist(), train_df[\"label\"].tolist(), tokenizer, config, is_train=True),\n",
    "    batch_size=config.batch_size, shuffle=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    ClassificationDataset(val_df[\"conversation\"].tolist(), val_df[\"label\"].tolist(), tokenizer, config, is_train=False),\n",
    "    batch_size=config.batch_size\n",
    ")\n",
    "# debug(train_loader, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e2bd0365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): StandardBertModel(\n",
       "    (token_emb): Embedding(35000, 256)\n",
       "    (pos_emb): Embedding(128, 256)\n",
       "    (seg_emb): Embedding(2, 256)\n",
       "    (norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x ModuleDict(\n",
       "        (mha): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.3, inplace=False)\n",
       "          (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (4): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (pooler): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (1): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (classifier): Linear(in_features=256, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. 모델 초기화\n",
    "backbone = StandardBertModel(config)\n",
    "model = BertForSequenceClassification(backbone, config, num_labels=5).to(device)\n",
    "model.apply(init_weights) # 가중치 안정화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4e74ac63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  [Train] Loss: 1.6100 | Acc: 0.1980\n",
      "  [Val]   Loss: 1.6093 | Acc: 0.2000 | F1: 0.0667\n",
      "  >> Best F1 Updated! (0.0667)\n",
      "  >> Best Loss Updated! (1.6093)\n",
      "  >> Best model Updated! (0.0667, 1.6093)\n",
      "--------------------------------------------------\n",
      "Epoch 2/100\n",
      "  [Train] Loss: 1.6094 | Acc: 0.2084\n",
      "  [Val]   Loss: 1.6081 | Acc: 0.2000 | F1: 0.0667\n",
      "  >> Best Loss Updated! (1.6081)\n",
      "  >> Best model Updated! (0.0667, 1.6081)\n",
      "--------------------------------------------------\n",
      "Epoch 3/100\n",
      "  [Train] Loss: 1.6054 | Acc: 0.2284\n",
      "  [Val]   Loss: 1.5731 | Acc: 0.2730 | F1: 0.1846\n",
      "  >> Best F1 Updated! (0.1846)\n",
      "  >> Best Loss Updated! (1.5731)\n",
      "  >> Best model Updated! (0.1846, 1.5731)\n",
      "--------------------------------------------------\n",
      "Epoch 4/100\n",
      "  [Train] Loss: 1.4133 | Acc: 0.4320\n",
      "  [Val]   Loss: 1.2820 | Acc: 0.5787 | F1: 0.5139\n",
      "  >> Best F1 Updated! (0.5139)\n",
      "  >> Best Loss Updated! (1.2820)\n",
      "  >> Best model Updated! (0.5139, 1.2820)\n",
      "--------------------------------------------------\n",
      "Epoch 5/100\n",
      "  [Train] Loss: 1.1664 | Acc: 0.5868\n",
      "  [Val]   Loss: 1.0992 | Acc: 0.6629 | F1: 0.6090\n",
      "  >> Best F1 Updated! (0.6090)\n",
      "  >> Best Loss Updated! (1.0992)\n",
      "  >> Best model Updated! (0.6090, 1.0992)\n",
      "--------------------------------------------------\n",
      "Epoch 6/100\n",
      "  [Train] Loss: 0.9943 | Acc: 0.7152\n",
      "  [Val]   Loss: 0.9467 | Acc: 0.7404 | F1: 0.7204\n",
      "  >> Best F1 Updated! (0.7204)\n",
      "  >> Best Loss Updated! (0.9467)\n",
      "  >> Best model Updated! (0.7204, 0.9467)\n",
      "--------------------------------------------------\n",
      "Epoch 7/100\n",
      "  [Train] Loss: 0.8563 | Acc: 0.8020\n",
      "  [Val]   Loss: 0.8601 | Acc: 0.7865 | F1: 0.7819\n",
      "  >> Best F1 Updated! (0.7819)\n",
      "  >> Best Loss Updated! (0.8601)\n",
      "  >> Best model Updated! (0.7819, 0.8601)\n",
      "--------------------------------------------------\n",
      "Epoch 8/100\n",
      "  [Train] Loss: 0.7455 | Acc: 0.8531\n",
      "  [Val]   Loss: 0.8418 | Acc: 0.7876 | F1: 0.7872\n",
      "  >> Best F1 Updated! (0.7872)\n",
      "  >> Best Loss Updated! (0.8418)\n",
      "  >> Best model Updated! (0.7872, 0.8418)\n",
      "--------------------------------------------------\n",
      "Epoch 9/100\n",
      "  [Train] Loss: 0.6541 | Acc: 0.9006\n",
      "  [Val]   Loss: 0.8150 | Acc: 0.8067 | F1: 0.8076\n",
      "  >> Best F1 Updated! (0.8076)\n",
      "  >> Best Loss Updated! (0.8150)\n",
      "  >> Best model Updated! (0.8076, 0.8150)\n",
      "--------------------------------------------------\n",
      "Epoch 10/100\n",
      "  [Train] Loss: 0.6034 | Acc: 0.9199\n",
      "  [Val]   Loss: 0.8343 | Acc: 0.8112 | F1: 0.8113\n",
      "  >> Best F1 Updated! (0.8113)\n",
      "  >> Best model Updated! (0.8113, 0.8343)\n",
      "--------------------------------------------------\n",
      "Epoch 11/100\n",
      "  [Train] Loss: 0.5578 | Acc: 0.9388\n",
      "  [Val]   Loss: 0.8268 | Acc: 0.8180 | F1: 0.8187\n",
      "  >> Best F1 Updated! (0.8187)\n",
      "  >> Best model Updated! (0.8187, 0.8268)\n",
      "--------------------------------------------------\n",
      "Epoch 12/100\n",
      "  [Train] Loss: 0.5317 | Acc: 0.9486\n",
      "  [Val]   Loss: 0.8461 | Acc: 0.8191 | F1: 0.8205\n",
      "  >> Best F1 Updated! (0.8205)\n",
      "  >> Best model Updated! (0.8205, 0.8461)\n",
      "--------------------------------------------------\n",
      "Epoch 13/100\n",
      "  [Train] Loss: 0.5084 | Acc: 0.9556\n",
      "  [Val]   Loss: 0.8603 | Acc: 0.8191 | F1: 0.8221\n",
      "  >> Best F1 Updated! (0.8221)\n",
      "  >> Best model Updated! (0.8221, 0.8603)\n",
      "--------------------------------------------------\n",
      "Epoch 14/100\n",
      "  [Train] Loss: 0.4766 | Acc: 0.9643\n",
      "  [Val]   Loss: 0.8592 | Acc: 0.8180 | F1: 0.8191\n",
      "  >> EarlyStopping counter: 1 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 15/100\n",
      "  [Train] Loss: 0.4624 | Acc: 0.9705\n",
      "  [Val]   Loss: 0.9222 | Acc: 0.8101 | F1: 0.8155\n",
      "  >> EarlyStopping counter: 2 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 16/100\n",
      "  [Train] Loss: 0.4474 | Acc: 0.9778\n",
      "  [Val]   Loss: 0.9099 | Acc: 0.8067 | F1: 0.8077\n",
      "  >> EarlyStopping counter: 3 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 17/100\n",
      "  [Train] Loss: 0.4303 | Acc: 0.9846\n",
      "  [Val]   Loss: 0.9324 | Acc: 0.8056 | F1: 0.8059\n",
      "  >> EarlyStopping counter: 4 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 18/100\n",
      "  [Train] Loss: 0.4296 | Acc: 0.9823\n",
      "  [Val]   Loss: 0.9532 | Acc: 0.7933 | F1: 0.7949\n",
      "  >> EarlyStopping counter: 5 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 19/100\n",
      "  [Train] Loss: 0.4308 | Acc: 0.9820\n",
      "  [Val]   Loss: 0.9608 | Acc: 0.7966 | F1: 0.7990\n",
      "  >> EarlyStopping counter: 6 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 20/100\n",
      "  [Train] Loss: 0.4245 | Acc: 0.9857\n",
      "  [Val]   Loss: 0.9960 | Acc: 0.7876 | F1: 0.7905\n",
      "  >> EarlyStopping counter: 7 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 21/100\n",
      "  [Train] Loss: 0.4278 | Acc: 0.9829\n",
      "  [Val]   Loss: 0.9352 | Acc: 0.8034 | F1: 0.8058\n",
      "  >> EarlyStopping counter: 8 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 22/100\n",
      "  [Train] Loss: 0.4223 | Acc: 0.9871\n",
      "  [Val]   Loss: 0.9988 | Acc: 0.7910 | F1: 0.7937\n",
      "  >> EarlyStopping counter: 9 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 23/100\n",
      "  [Train] Loss: 0.4197 | Acc: 0.9879\n",
      "  [Val]   Loss: 0.9965 | Acc: 0.7989 | F1: 0.8007\n",
      "  >> EarlyStopping counter: 10 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 24/100\n",
      "  [Train] Loss: 0.4123 | Acc: 0.9907\n",
      "  [Val]   Loss: 0.9469 | Acc: 0.8135 | F1: 0.8143\n",
      "  >> EarlyStopping counter: 11 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 25/100\n",
      "  [Train] Loss: 0.4159 | Acc: 0.9902\n",
      "  [Val]   Loss: 0.9602 | Acc: 0.8034 | F1: 0.8047\n",
      "  >> EarlyStopping counter: 12 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 26/100\n",
      "  [Train] Loss: 0.4178 | Acc: 0.9902\n",
      "  [Val]   Loss: 1.0132 | Acc: 0.7843 | F1: 0.7860\n",
      "  >> EarlyStopping counter: 13 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 27/100\n",
      "  [Train] Loss: 0.4107 | Acc: 0.9916\n",
      "  [Val]   Loss: 0.9358 | Acc: 0.8146 | F1: 0.8157\n",
      "  >> EarlyStopping counter: 14 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 28/100\n",
      "  [Train] Loss: 0.4158 | Acc: 0.9896\n",
      "  [Val]   Loss: 0.9482 | Acc: 0.8157 | F1: 0.8172\n",
      "  >> EarlyStopping counter: 15 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 29/100\n",
      "  [Train] Loss: 0.4141 | Acc: 0.9902\n",
      "  [Val]   Loss: 0.9541 | Acc: 0.8180 | F1: 0.8199\n",
      "  >> EarlyStopping counter: 16 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 30/100\n",
      "  [Train] Loss: 0.4082 | Acc: 0.9927\n",
      "  [Val]   Loss: 0.9940 | Acc: 0.8034 | F1: 0.8058\n",
      "  >> EarlyStopping counter: 17 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 31/100\n",
      "  [Train] Loss: 0.4106 | Acc: 0.9913\n",
      "  [Val]   Loss: 1.0081 | Acc: 0.8045 | F1: 0.8077\n",
      "  >> EarlyStopping counter: 18 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 32/100\n",
      "  [Train] Loss: 0.4104 | Acc: 0.9935\n",
      "  [Val]   Loss: 0.9556 | Acc: 0.8157 | F1: 0.8160\n",
      "  >> EarlyStopping counter: 19 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 33/100\n",
      "  [Train] Loss: 0.4112 | Acc: 0.9921\n",
      "  [Val]   Loss: 0.9882 | Acc: 0.8101 | F1: 0.8105\n",
      "  >> EarlyStopping counter: 20 out of 20\n",
      "성능 개선이 없어 33 에포크에서 학습을 조기 종료합니다.\n",
      "학습이 완료되었습니다. 최종 지표 분석을 시작합니다.\n",
      ">> 최고 성능 모델 복원 완료 (Epoch 13)\n",
      "val_acc:0.8191011235955056, val_acc: 0.8191011235955056, f1:0.8220512121937646\n",
      ">> Confusion Matrix image saved to ./results/monologg_koelectra-base-v3-discriminator_d256_f1024_l4/final_best_confusion_matrix.png\n",
      ">> [Success] 시각화 결과가 저장되었습니다: ./results/monologg_koelectra-base-v3-discriminator\n",
      ">> 모든 최종 결과물이 ./results/monologg_koelectra-base-v3-discriminator 에 저장되었습니다.\n",
      "CPU times: user 2min 58s, sys: 16.3 s, total: 3min 14s\n",
      "Wall time: 3min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = train(\n",
    "   config, model, train_loader, val_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "51adba9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 가중치 로드 완료 (Epoch: 12, F1: 0.8221)\n",
      "모델 가중치 로드 완료 (Epoch: 12, F1: 0.8221)\n",
      "사전학습된 토크나이저 로드 중: monologg/koelectra-base-v3-discriminator\n",
      "추론 시작... (테스트 데이터: 500개)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 16/16 [00:00<00:00, 49.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최종 결과 저장 완료: ./dataset/monologg_koelectra-base-v3-discriminator_d256_f1024_l4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>class</th>\n",
       "      <th>text_label</th>\n",
       "      <th>confidence</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>conversation</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t_000</td>\n",
       "      <td>1</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>0.9128</td>\n",
       "      <td>아가씨 담배 ##한 ##갑 ##주 ##소 네 원 ##입니다 어 네 지갑 ##어 ##...</td>\n",
       "      <td>아가씨 담배한갑주소 네 4500원입니다 어 네 지갑어디갔지 에이 버스에서 잃어버렸나...</td>\n",
       "      <td>아가씨 담배한갑주소 네 원입니다 어 네 지갑어디갔지 에이 버스에서 잃어버렸나보네 그...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t_001</td>\n",
       "      <td>2</td>\n",
       "      <td>직장 내 괴롭힘 대화</td>\n",
       "      <td>0.9197</td>\n",
       "      <td>우리 ##팀 ##에 ##서 다른 ##팀 ##으로 갈 사람 없 ##나 그럼 영지 ##...</td>\n",
       "      <td>우리팀에서 다른팀으로 갈 사람 없나? 그럼 영지씨가 가는건 어때?&nbsp;&nbsp;네? 제가요? ...</td>\n",
       "      <td>우리팀에서 다른팀으로 갈 사람 없나 그럼 영지씨가 가는건 어때 네 제가요 그렇지 달...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t_002</td>\n",
       "      <td>2</td>\n",
       "      <td>직장 내 괴롭힘 대화</td>\n",
       "      <td>0.8995</td>\n",
       "      <td>너 오늘 그게 뭐 ##야 네 제 ##가 뭘 잘못 ##했 ##나 ##요 제대로 좀 하...</td>\n",
       "      <td>너 오늘 그게 뭐야 네 제가 뭘 잘못했나요.? 제대로 좀 하지 네 똑바로 좀 하지 ...</td>\n",
       "      <td>너 오늘 그게 뭐야 네 제가 뭘 잘못했나요 제대로 좀 하지 네 똑바로 좀 하지 행실...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t_003</td>\n",
       "      <td>3</td>\n",
       "      <td>기타 괴롭힘 대화</td>\n",
       "      <td>0.8941</td>\n",
       "      <td>이거 들어 ##바 와 이 노래 진짜 좋 ##다 그치 요즘 이 것 ##만 들어 진짜 ...</td>\n",
       "      <td>이거 들어바 와 이 노래 진짜 좋다 그치 요즘 이 것만 들어 진짜 너무 좋다 내가 ...</td>\n",
       "      <td>이거 들어바 와 이 노래 진짜 좋다 그치 요즘 이 것만 들어 진짜 너무 좋다 내가 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t_004</td>\n",
       "      <td>3</td>\n",
       "      <td>기타 괴롭힘 대화</td>\n",
       "      <td>0.9081</td>\n",
       "      <td>아무튼 앞 ##으로 니 ##가 내 와이파이 ##야 응 와이파이 온 켰 ##어 반말 ...</td>\n",
       "      <td>아무튼 앞으로 니가 내 와이파이야. .응 와이파이 온. 켰어. 반말? 주인님이라고도...</td>\n",
       "      <td>아무튼 앞으로 니가 내 와이파이야 응 와이파이 온 켰어 반말 주인님이라고도 말해야지...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>t_495</td>\n",
       "      <td>2</td>\n",
       "      <td>직장 내 괴롭힘 대화</td>\n",
       "      <td>0.9214</td>\n",
       "      <td>미나 ##씨 휴가 결제 올리 ##기 전 ##에 저 ##랑 상의 ##하라 ##고 말 ...</td>\n",
       "      <td>미나씨 휴가 결제 올리기 전에 저랑 상의하라고 말한거 기억해요? 네 합니다. 보고서...</td>\n",
       "      <td>미나씨 휴가 결제 올리기 전에 저랑 상의하라고 말한거 기억해요 네 합니다 보고서를 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>t_496</td>\n",
       "      <td>1</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>0.4080</td>\n",
       "      <td>교수 ##님 제 논문 ##에 제 이름 ##이 없 ##나 ##요 아 무슨 논문 ##말...</td>\n",
       "      <td>교수님 제 논문에 제 이름이 없나요?&nbsp;&nbsp;아 무슨 논문말이야?&nbsp;&nbsp;지난 번 냈던 논문이...</td>\n",
       "      <td>교수님 제 논문에 제 이름이 없나요 아 무슨 논문말이야 지난 번 냈던 논문이요 그거...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>t_497</td>\n",
       "      <td>1</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>0.9090</td>\n",
       "      <td>야 너 네 저 ##요 그래 너 왜 ##요 돈 ##좀 줘 ##봐 돈 없 ##어요 돈 ...</td>\n",
       "      <td>야 너&nbsp;&nbsp;네 저요? 그래 너 왜요 돈좀 줘봐&nbsp;&nbsp;돈 없어요 돈이 왜 없어 지갑은 폼이...</td>\n",
       "      <td>야 너 네 저요 그래 너 왜요 돈좀 줘봐 돈 없어요 돈이 왜 없어 지갑은 폼이니 진...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>t_498</td>\n",
       "      <td>2</td>\n",
       "      <td>직장 내 괴롭힘 대화</td>\n",
       "      <td>0.8755</td>\n",
       "      <td>야 너 빨리 안 뛰어 ##와 너 이 환자 제대로 봤 ##어 안 봤 ##어 어제 저녁...</td>\n",
       "      <td>야 너 빨리 안 뛰어와? 너 이 환자 제대로 봤어 안 봤어 어제 저녁부터 계속 보다...</td>\n",
       "      <td>야 너 빨리 안 뛰어와 너 이 환자 제대로 봤어 안 봤어 어제 저녁부터 계속 보다가...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>t_499</td>\n",
       "      <td>0</td>\n",
       "      <td>협박 대화</td>\n",
       "      <td>0.9244</td>\n",
       "      <td>엄마 저 그 돈 안 ##해 ##주 ##시 ##면 정말 큰일 ##나 ##요 이유 ##...</td>\n",
       "      <td>엄마 저 그 돈 안해주시면 정말 큰일나요.&nbsp;&nbsp;이유도 말하지 않고. 몇번째니 경민아....</td>\n",
       "      <td>엄마 저 그 돈 안해주시면 정말 큰일나요 이유도 말하지 않고 몇번째니 경민아 엄마 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       idx  class   text_label  confidence  \\\n",
       "0    t_000      1        갈취 대화      0.9128   \n",
       "1    t_001      2  직장 내 괴롭힘 대화      0.9197   \n",
       "2    t_002      2  직장 내 괴롭힘 대화      0.8995   \n",
       "3    t_003      3    기타 괴롭힘 대화      0.8941   \n",
       "4    t_004      3    기타 괴롭힘 대화      0.9081   \n",
       "..     ...    ...          ...         ...   \n",
       "495  t_495      2  직장 내 괴롭힘 대화      0.9214   \n",
       "496  t_496      1        갈취 대화      0.4080   \n",
       "497  t_497      1        갈취 대화      0.9090   \n",
       "498  t_498      2  직장 내 괴롭힘 대화      0.8755   \n",
       "499  t_499      0        협박 대화      0.9244   \n",
       "\n",
       "                                        tokenized_text  \\\n",
       "0    아가씨 담배 ##한 ##갑 ##주 ##소 네 원 ##입니다 어 네 지갑 ##어 ##...   \n",
       "1    우리 ##팀 ##에 ##서 다른 ##팀 ##으로 갈 사람 없 ##나 그럼 영지 ##...   \n",
       "2    너 오늘 그게 뭐 ##야 네 제 ##가 뭘 잘못 ##했 ##나 ##요 제대로 좀 하...   \n",
       "3    이거 들어 ##바 와 이 노래 진짜 좋 ##다 그치 요즘 이 것 ##만 들어 진짜 ...   \n",
       "4    아무튼 앞 ##으로 니 ##가 내 와이파이 ##야 응 와이파이 온 켰 ##어 반말 ...   \n",
       "..                                                 ...   \n",
       "495  미나 ##씨 휴가 결제 올리 ##기 전 ##에 저 ##랑 상의 ##하라 ##고 말 ...   \n",
       "496  교수 ##님 제 논문 ##에 제 이름 ##이 없 ##나 ##요 아 무슨 논문 ##말...   \n",
       "497  야 너 네 저 ##요 그래 너 왜 ##요 돈 ##좀 줘 ##봐 돈 없 ##어요 돈 ...   \n",
       "498  야 너 빨리 안 뛰어 ##와 너 이 환자 제대로 봤 ##어 안 봤 ##어 어제 저녁...   \n",
       "499  엄마 저 그 돈 안 ##해 ##주 ##시 ##면 정말 큰일 ##나 ##요 이유 ##...   \n",
       "\n",
       "                                          conversation  \\\n",
       "0    아가씨 담배한갑주소 네 4500원입니다 어 네 지갑어디갔지 에이 버스에서 잃어버렸나...   \n",
       "1    우리팀에서 다른팀으로 갈 사람 없나? 그럼 영지씨가 가는건 어때?  네? 제가요? ...   \n",
       "2    너 오늘 그게 뭐야 네 제가 뭘 잘못했나요.? 제대로 좀 하지 네 똑바로 좀 하지 ...   \n",
       "3    이거 들어바 와 이 노래 진짜 좋다 그치 요즘 이 것만 들어 진짜 너무 좋다 내가 ...   \n",
       "4    아무튼 앞으로 니가 내 와이파이야. .응 와이파이 온. 켰어. 반말? 주인님이라고도...   \n",
       "..                                                 ...   \n",
       "495  미나씨 휴가 결제 올리기 전에 저랑 상의하라고 말한거 기억해요? 네 합니다. 보고서...   \n",
       "496  교수님 제 논문에 제 이름이 없나요?  아 무슨 논문말이야?  지난 번 냈던 논문이...   \n",
       "497  야 너  네 저요? 그래 너 왜요 돈좀 줘봐  돈 없어요 돈이 왜 없어 지갑은 폼이...   \n",
       "498  야 너 빨리 안 뛰어와? 너 이 환자 제대로 봤어 안 봤어 어제 저녁부터 계속 보다...   \n",
       "499  엄마 저 그 돈 안해주시면 정말 큰일나요.  이유도 말하지 않고. 몇번째니 경민아....   \n",
       "\n",
       "                                          cleaned_text  \n",
       "0    아가씨 담배한갑주소 네 원입니다 어 네 지갑어디갔지 에이 버스에서 잃어버렸나보네 그...  \n",
       "1    우리팀에서 다른팀으로 갈 사람 없나 그럼 영지씨가 가는건 어때 네 제가요 그렇지 달...  \n",
       "2    너 오늘 그게 뭐야 네 제가 뭘 잘못했나요 제대로 좀 하지 네 똑바로 좀 하지 행실...  \n",
       "3    이거 들어바 와 이 노래 진짜 좋다 그치 요즘 이 것만 들어 진짜 너무 좋다 내가 ...  \n",
       "4    아무튼 앞으로 니가 내 와이파이야 응 와이파이 온 켰어 반말 주인님이라고도 말해야지...  \n",
       "..                                                 ...  \n",
       "495  미나씨 휴가 결제 올리기 전에 저랑 상의하라고 말한거 기억해요 네 합니다 보고서를 ...  \n",
       "496  교수님 제 논문에 제 이름이 없나요 아 무슨 논문말이야 지난 번 냈던 논문이요 그거...  \n",
       "497  야 너 네 저요 그래 너 왜요 돈좀 줘봐 돈 없어요 돈이 왜 없어 지갑은 폼이니 진...  \n",
       "498  야 너 빨리 안 뛰어와 너 이 환자 제대로 봤어 안 봤어 어제 저녁부터 계속 보다가...  \n",
       "499  엄마 저 그 돈 안해주시면 정말 큰일나요 이유도 말하지 않고 몇번째니 경민아 엄마 ...  \n",
       "\n",
       "[500 rows x 7 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_and_predict(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d336753",
   "metadata": {},
   "source": [
    "### klue/bert-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b4dd6b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.tokenizer_type = 'pretrained' \n",
    "config.pretrained_model_name = \"klue/bert-base\"\n",
    "config.num_layers = 4\n",
    "config.d_model = 256\n",
    "config.ff_dim = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aca5c152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사전학습된 토크나이저 로드 중: klue/bert-base\n",
      "[Config Finalized] Vocab: 32000\n",
      " >> IDs - PAD:0, UNK:1, CLS:2, SEP:3, MASK:4\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(config, df[\"conversation\"].tolist())\n",
    "config.update_from_tokenizer(tokenizer) # Vocab Size 동기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7f62c5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df[\"label\"])\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    ClassificationDataset(train_df[\"conversation\"].tolist(), train_df[\"label\"].tolist(), tokenizer, config, is_train=True),\n",
    "    batch_size=config.batch_size, shuffle=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    ClassificationDataset(val_df[\"conversation\"].tolist(), val_df[\"label\"].tolist(), tokenizer, config, is_train=False),\n",
    "    batch_size=config.batch_size\n",
    ")\n",
    "# debug(train_loader, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4312a202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): StandardBertModel(\n",
       "    (token_emb): Embedding(32000, 256)\n",
       "    (pos_emb): Embedding(128, 256)\n",
       "    (seg_emb): Embedding(2, 256)\n",
       "    (norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x ModuleDict(\n",
       "        (mha): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.3, inplace=False)\n",
       "          (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (4): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (pooler): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (1): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (classifier): Linear(in_features=256, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. 모델 초기화\n",
    "backbone = StandardBertModel(config)\n",
    "model = BertForSequenceClassification(backbone, config, num_labels=5).to(device)\n",
    "model.apply(init_weights) # 가중치 안정화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9b0752db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  [Train] Loss: 1.6101 | Acc: 0.1994\n",
      "  [Val]   Loss: 1.6093 | Acc: 0.2000 | F1: 0.0667\n",
      "  >> Best F1 Updated! (0.0667)\n",
      "  >> Best Loss Updated! (1.6093)\n",
      "  >> Best model Updated! (0.0667, 1.6093)\n",
      "--------------------------------------------------\n",
      "Epoch 2/100\n",
      "  [Train] Loss: 1.6094 | Acc: 0.2003\n",
      "  [Val]   Loss: 1.6087 | Acc: 0.2000 | F1: 0.0667\n",
      "  >> Best Loss Updated! (1.6087)\n",
      "  >> Best model Updated! (0.0667, 1.6087)\n",
      "--------------------------------------------------\n",
      "Epoch 3/100\n",
      "  [Train] Loss: 1.6076 | Acc: 0.2295\n",
      "  [Val]   Loss: 1.5909 | Acc: 0.3517 | F1: 0.2109\n",
      "  >> Best F1 Updated! (0.2109)\n",
      "  >> Best Loss Updated! (1.5909)\n",
      "  >> Best model Updated! (0.2109, 1.5909)\n",
      "--------------------------------------------------\n",
      "Epoch 4/100\n",
      "  [Train] Loss: 1.4035 | Acc: 0.4857\n",
      "  [Val]   Loss: 1.2067 | Acc: 0.6169 | F1: 0.5533\n",
      "  >> Best F1 Updated! (0.5533)\n",
      "  >> Best Loss Updated! (1.2067)\n",
      "  >> Best model Updated! (0.5533, 1.2067)\n",
      "--------------------------------------------------\n",
      "Epoch 5/100\n",
      "  [Train] Loss: 1.0947 | Acc: 0.6596\n",
      "  [Val]   Loss: 1.0177 | Acc: 0.7112 | F1: 0.6798\n",
      "  >> Best F1 Updated! (0.6798)\n",
      "  >> Best Loss Updated! (1.0177)\n",
      "  >> Best model Updated! (0.6798, 1.0177)\n",
      "--------------------------------------------------\n",
      "Epoch 6/100\n",
      "  [Train] Loss: 0.9243 | Acc: 0.7604\n",
      "  [Val]   Loss: 0.9170 | Acc: 0.7551 | F1: 0.7495\n",
      "  >> Best F1 Updated! (0.7495)\n",
      "  >> Best Loss Updated! (0.9170)\n",
      "  >> Best model Updated! (0.7495, 0.9170)\n",
      "--------------------------------------------------\n",
      "Epoch 7/100\n",
      "  [Train] Loss: 0.7750 | Acc: 0.8441\n",
      "  [Val]   Loss: 0.8164 | Acc: 0.8045 | F1: 0.8026\n",
      "  >> Best F1 Updated! (0.8026)\n",
      "  >> Best Loss Updated! (0.8164)\n",
      "  >> Best model Updated! (0.8026, 0.8164)\n",
      "--------------------------------------------------\n",
      "Epoch 8/100\n",
      "  [Train] Loss: 0.6722 | Acc: 0.8865\n",
      "  [Val]   Loss: 0.8560 | Acc: 0.7910 | F1: 0.7910\n",
      "  >> EarlyStopping counter: 1 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 9/100\n",
      "  [Train] Loss: 0.5974 | Acc: 0.9213\n",
      "  [Val]   Loss: 0.7962 | Acc: 0.8225 | F1: 0.8255\n",
      "  >> Best F1 Updated! (0.8255)\n",
      "  >> Best Loss Updated! (0.7962)\n",
      "  >> Best model Updated! (0.8255, 0.7962)\n",
      "--------------------------------------------------\n",
      "Epoch 10/100\n",
      "  [Train] Loss: 0.5548 | Acc: 0.9376\n",
      "  [Val]   Loss: 0.8181 | Acc: 0.8225 | F1: 0.8216\n",
      "  >> EarlyStopping counter: 1 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 11/100\n",
      "  [Train] Loss: 0.5201 | Acc: 0.9514\n",
      "  [Val]   Loss: 0.8059 | Acc: 0.8247 | F1: 0.8257\n",
      "  >> Best F1 Updated! (0.8257)\n",
      "  >> Best model Updated! (0.8257, 0.8059)\n",
      "--------------------------------------------------\n",
      "Epoch 12/100\n",
      "  [Train] Loss: 0.4913 | Acc: 0.9657\n",
      "  [Val]   Loss: 0.8442 | Acc: 0.8191 | F1: 0.8221\n",
      "  >> EarlyStopping counter: 1 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 13/100\n",
      "  [Train] Loss: 0.4565 | Acc: 0.9753\n",
      "  [Val]   Loss: 0.8700 | Acc: 0.8191 | F1: 0.8229\n",
      "  >> EarlyStopping counter: 2 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 14/100\n",
      "  [Train] Loss: 0.4462 | Acc: 0.9789\n",
      "  [Val]   Loss: 0.8783 | Acc: 0.8270 | F1: 0.8277\n",
      "  >> Best F1 Updated! (0.8277)\n",
      "  >> Best model Updated! (0.8277, 0.8783)\n",
      "--------------------------------------------------\n",
      "Epoch 15/100\n",
      "  [Train] Loss: 0.4422 | Acc: 0.9772\n",
      "  [Val]   Loss: 0.8923 | Acc: 0.8180 | F1: 0.8207\n",
      "  >> EarlyStopping counter: 1 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 16/100\n",
      "  [Train] Loss: 0.4375 | Acc: 0.9820\n",
      "  [Val]   Loss: 0.8976 | Acc: 0.8236 | F1: 0.8251\n",
      "  >> EarlyStopping counter: 2 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 17/100\n",
      "  [Train] Loss: 0.4308 | Acc: 0.9843\n",
      "  [Val]   Loss: 0.9190 | Acc: 0.8124 | F1: 0.8129\n",
      "  >> EarlyStopping counter: 3 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 18/100\n",
      "  [Train] Loss: 0.4213 | Acc: 0.9865\n",
      "  [Val]   Loss: 0.9317 | Acc: 0.8124 | F1: 0.8129\n",
      "  >> EarlyStopping counter: 4 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 19/100\n",
      "  [Train] Loss: 0.4322 | Acc: 0.9823\n",
      "  [Val]   Loss: 0.9320 | Acc: 0.8101 | F1: 0.8109\n",
      "  >> EarlyStopping counter: 5 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 20/100\n",
      "  [Train] Loss: 0.4242 | Acc: 0.9854\n",
      "  [Val]   Loss: 0.9476 | Acc: 0.8045 | F1: 0.8054\n",
      "  >> EarlyStopping counter: 6 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 21/100\n",
      "  [Train] Loss: 0.4211 | Acc: 0.9890\n",
      "  [Val]   Loss: 0.9499 | Acc: 0.8079 | F1: 0.8092\n",
      "  >> EarlyStopping counter: 7 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 22/100\n",
      "  [Train] Loss: 0.4244 | Acc: 0.9865\n",
      "  [Val]   Loss: 0.9892 | Acc: 0.7978 | F1: 0.7998\n",
      "  >> EarlyStopping counter: 8 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 23/100\n",
      "  [Train] Loss: 0.4252 | Acc: 0.9865\n",
      "  [Val]   Loss: 0.9818 | Acc: 0.7989 | F1: 0.8003\n",
      "  >> EarlyStopping counter: 9 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 24/100\n",
      "  [Train] Loss: 0.4220 | Acc: 0.9876\n",
      "  [Val]   Loss: 0.9561 | Acc: 0.8169 | F1: 0.8179\n",
      "  >> EarlyStopping counter: 10 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 25/100\n",
      "  [Train] Loss: 0.4152 | Acc: 0.9907\n",
      "  [Val]   Loss: 0.9699 | Acc: 0.8090 | F1: 0.8109\n",
      "  >> EarlyStopping counter: 11 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 26/100\n",
      "  [Train] Loss: 0.4160 | Acc: 0.9896\n",
      "  [Val]   Loss: 0.9788 | Acc: 0.7989 | F1: 0.7990\n",
      "  >> EarlyStopping counter: 12 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 27/100\n",
      "  [Train] Loss: 0.4140 | Acc: 0.9910\n",
      "  [Val]   Loss: 0.9924 | Acc: 0.8022 | F1: 0.8040\n",
      "  >> EarlyStopping counter: 13 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 28/100\n",
      "  [Train] Loss: 0.4151 | Acc: 0.9896\n",
      "  [Val]   Loss: 0.9856 | Acc: 0.8056 | F1: 0.8068\n",
      "  >> EarlyStopping counter: 14 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 29/100\n",
      "  [Train] Loss: 0.4099 | Acc: 0.9921\n",
      "  [Val]   Loss: 0.9536 | Acc: 0.8225 | F1: 0.8228\n",
      "  >> EarlyStopping counter: 15 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 30/100\n",
      "  [Train] Loss: 0.4120 | Acc: 0.9916\n",
      "  [Val]   Loss: 0.9591 | Acc: 0.8180 | F1: 0.8182\n",
      "  >> EarlyStopping counter: 16 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 31/100\n",
      "  [Train] Loss: 0.4027 | Acc: 0.9947\n",
      "  [Val]   Loss: 0.9844 | Acc: 0.8079 | F1: 0.8070\n",
      "  >> EarlyStopping counter: 17 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 32/100\n",
      "  [Train] Loss: 0.4130 | Acc: 0.9916\n",
      "  [Val]   Loss: 0.9740 | Acc: 0.8124 | F1: 0.8142\n",
      "  >> EarlyStopping counter: 18 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 33/100\n",
      "  [Train] Loss: 0.4104 | Acc: 0.9919\n",
      "  [Val]   Loss: 0.9976 | Acc: 0.8056 | F1: 0.8056\n",
      "  >> EarlyStopping counter: 19 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 34/100\n",
      "  [Train] Loss: 0.4035 | Acc: 0.9952\n",
      "  [Val]   Loss: 0.9818 | Acc: 0.8101 | F1: 0.8107\n",
      "  >> EarlyStopping counter: 20 out of 20\n",
      "성능 개선이 없어 34 에포크에서 학습을 조기 종료합니다.\n",
      "학습이 완료되었습니다. 최종 지표 분석을 시작합니다.\n",
      ">> 최고 성능 모델 복원 완료 (Epoch 14)\n",
      "val_acc:0.8269662921348314, val_acc: 0.8269662921348314, f1:0.8277342834263356\n",
      ">> Confusion Matrix image saved to ./results/klue_bert-base_d256_f1024_l4/final_best_confusion_matrix.png\n",
      ">> [Success] 시각화 결과가 저장되었습니다: ./results/klue_bert-base\n",
      ">> 모든 최종 결과물이 ./results/klue_bert-base 에 저장되었습니다.\n",
      "CPU times: user 3min 4s, sys: 15.6 s, total: 3min 20s\n",
      "Wall time: 3min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = train(\n",
    "   config, model, train_loader, val_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "238117ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 가중치 로드 완료 (Epoch: 13, F1: 0.8277)\n",
      "모델 가중치 로드 완료 (Epoch: 13, F1: 0.8277)\n",
      "사전학습된 토크나이저 로드 중: klue/bert-base\n",
      "추론 시작... (테스트 데이터: 500개)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 16/16 [00:00<00:00, 47.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최종 결과 저장 완료: ./dataset/klue_bert-base_d256_f1024_l4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>class</th>\n",
       "      <th>text_label</th>\n",
       "      <th>confidence</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>conversation</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t_000</td>\n",
       "      <td>1</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>0.9409</td>\n",
       "      <td>아가씨 담배 ##한 ##갑 ##주 ##소 네 원 ##입니다 어 네 지갑 ##어 ##...</td>\n",
       "      <td>아가씨 담배한갑주소 네 4500원입니다 어 네 지갑어디갔지 에이 버스에서 잃어버렸나...</td>\n",
       "      <td>아가씨 담배한갑주소 네 원입니다 어 네 지갑어디갔지 에이 버스에서 잃어버렸나보네 그...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t_001</td>\n",
       "      <td>2</td>\n",
       "      <td>직장 내 괴롭힘 대화</td>\n",
       "      <td>0.9411</td>\n",
       "      <td>우리 ##팀 ##에서 다른 ##팀 ##으로 갈 사람 없 ##나 그럼 영지 ##씨 #...</td>\n",
       "      <td>우리팀에서 다른팀으로 갈 사람 없나? 그럼 영지씨가 가는건 어때?&nbsp;&nbsp;네? 제가요? ...</td>\n",
       "      <td>우리팀에서 다른팀으로 갈 사람 없나 그럼 영지씨가 가는건 어때 네 제가요 그렇지 달...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t_002</td>\n",
       "      <td>2</td>\n",
       "      <td>직장 내 괴롭힘 대화</td>\n",
       "      <td>0.9412</td>\n",
       "      <td>너 오늘 그게 뭐 ##야 네 제 ##가 뭘 잘못 ##했 ##나 ##요 제대로 좀 하...</td>\n",
       "      <td>너 오늘 그게 뭐야 네 제가 뭘 잘못했나요.? 제대로 좀 하지 네 똑바로 좀 하지 ...</td>\n",
       "      <td>너 오늘 그게 뭐야 네 제가 뭘 잘못했나요 제대로 좀 하지 네 똑바로 좀 하지 행실...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t_003</td>\n",
       "      <td>4</td>\n",
       "      <td>일반 대화</td>\n",
       "      <td>0.8586</td>\n",
       "      <td>이거 들어 ##바 와 이 노래 진짜 좋 ##다 그치 요즘 이 것 ##만 들어 진짜 ...</td>\n",
       "      <td>이거 들어바 와 이 노래 진짜 좋다 그치 요즘 이 것만 들어 진짜 너무 좋다 내가 ...</td>\n",
       "      <td>이거 들어바 와 이 노래 진짜 좋다 그치 요즘 이 것만 들어 진짜 너무 좋다 내가 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t_004</td>\n",
       "      <td>2</td>\n",
       "      <td>직장 내 괴롭힘 대화</td>\n",
       "      <td>0.6463</td>\n",
       "      <td>아무튼 앞 ##으로 니 ##가 내 와이파이 ##야 응 와이파이 온 켰 ##어 반말 ...</td>\n",
       "      <td>아무튼 앞으로 니가 내 와이파이야. .응 와이파이 온. 켰어. 반말? 주인님이라고도...</td>\n",
       "      <td>아무튼 앞으로 니가 내 와이파이야 응 와이파이 온 켰어 반말 주인님이라고도 말해야지...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>t_495</td>\n",
       "      <td>2</td>\n",
       "      <td>직장 내 괴롭힘 대화</td>\n",
       "      <td>0.9401</td>\n",
       "      <td>미나 ##씨 휴가 결제 올리 ##기 전 ##에 저 ##랑 상의 ##하라 ##고 말 ...</td>\n",
       "      <td>미나씨 휴가 결제 올리기 전에 저랑 상의하라고 말한거 기억해요? 네 합니다. 보고서...</td>\n",
       "      <td>미나씨 휴가 결제 올리기 전에 저랑 상의하라고 말한거 기억해요 네 합니다 보고서를 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>t_496</td>\n",
       "      <td>2</td>\n",
       "      <td>직장 내 괴롭힘 대화</td>\n",
       "      <td>0.8516</td>\n",
       "      <td>교수 ##님 제 논문 ##에 제 이름 ##이 없 ##나 ##요 아 무슨 논문 ##말...</td>\n",
       "      <td>교수님 제 논문에 제 이름이 없나요?&nbsp;&nbsp;아 무슨 논문말이야?&nbsp;&nbsp;지난 번 냈던 논문이...</td>\n",
       "      <td>교수님 제 논문에 제 이름이 없나요 아 무슨 논문말이야 지난 번 냈던 논문이요 그거...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>t_497</td>\n",
       "      <td>1</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>0.9275</td>\n",
       "      <td>야 너 네 저 ##요 그래 너 왜 ##요 돈 ##좀 줘 ##봐 돈 없 ##어요 돈 ...</td>\n",
       "      <td>야 너&nbsp;&nbsp;네 저요? 그래 너 왜요 돈좀 줘봐&nbsp;&nbsp;돈 없어요 돈이 왜 없어 지갑은 폼이...</td>\n",
       "      <td>야 너 네 저요 그래 너 왜요 돈좀 줘봐 돈 없어요 돈이 왜 없어 지갑은 폼이니 진...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>t_498</td>\n",
       "      <td>2</td>\n",
       "      <td>직장 내 괴롭힘 대화</td>\n",
       "      <td>0.8289</td>\n",
       "      <td>야 너 빨리 안 뛰어 ##와 너 이 환자 제대로 봤 ##어 안 봤 ##어 어제 저녁...</td>\n",
       "      <td>야 너 빨리 안 뛰어와? 너 이 환자 제대로 봤어 안 봤어 어제 저녁부터 계속 보다...</td>\n",
       "      <td>야 너 빨리 안 뛰어와 너 이 환자 제대로 봤어 안 봤어 어제 저녁부터 계속 보다가...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>t_499</td>\n",
       "      <td>0</td>\n",
       "      <td>협박 대화</td>\n",
       "      <td>0.5832</td>\n",
       "      <td>엄마 저 그 돈 안 ##해 ##주 ##시 ##면 정말 큰일 ##나 ##요 이유 ##...</td>\n",
       "      <td>엄마 저 그 돈 안해주시면 정말 큰일나요.&nbsp;&nbsp;이유도 말하지 않고. 몇번째니 경민아....</td>\n",
       "      <td>엄마 저 그 돈 안해주시면 정말 큰일나요 이유도 말하지 않고 몇번째니 경민아 엄마 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       idx  class   text_label  confidence  \\\n",
       "0    t_000      1        갈취 대화      0.9409   \n",
       "1    t_001      2  직장 내 괴롭힘 대화      0.9411   \n",
       "2    t_002      2  직장 내 괴롭힘 대화      0.9412   \n",
       "3    t_003      4        일반 대화      0.8586   \n",
       "4    t_004      2  직장 내 괴롭힘 대화      0.6463   \n",
       "..     ...    ...          ...         ...   \n",
       "495  t_495      2  직장 내 괴롭힘 대화      0.9401   \n",
       "496  t_496      2  직장 내 괴롭힘 대화      0.8516   \n",
       "497  t_497      1        갈취 대화      0.9275   \n",
       "498  t_498      2  직장 내 괴롭힘 대화      0.8289   \n",
       "499  t_499      0        협박 대화      0.5832   \n",
       "\n",
       "                                        tokenized_text  \\\n",
       "0    아가씨 담배 ##한 ##갑 ##주 ##소 네 원 ##입니다 어 네 지갑 ##어 ##...   \n",
       "1    우리 ##팀 ##에서 다른 ##팀 ##으로 갈 사람 없 ##나 그럼 영지 ##씨 #...   \n",
       "2    너 오늘 그게 뭐 ##야 네 제 ##가 뭘 잘못 ##했 ##나 ##요 제대로 좀 하...   \n",
       "3    이거 들어 ##바 와 이 노래 진짜 좋 ##다 그치 요즘 이 것 ##만 들어 진짜 ...   \n",
       "4    아무튼 앞 ##으로 니 ##가 내 와이파이 ##야 응 와이파이 온 켰 ##어 반말 ...   \n",
       "..                                                 ...   \n",
       "495  미나 ##씨 휴가 결제 올리 ##기 전 ##에 저 ##랑 상의 ##하라 ##고 말 ...   \n",
       "496  교수 ##님 제 논문 ##에 제 이름 ##이 없 ##나 ##요 아 무슨 논문 ##말...   \n",
       "497  야 너 네 저 ##요 그래 너 왜 ##요 돈 ##좀 줘 ##봐 돈 없 ##어요 돈 ...   \n",
       "498  야 너 빨리 안 뛰어 ##와 너 이 환자 제대로 봤 ##어 안 봤 ##어 어제 저녁...   \n",
       "499  엄마 저 그 돈 안 ##해 ##주 ##시 ##면 정말 큰일 ##나 ##요 이유 ##...   \n",
       "\n",
       "                                          conversation  \\\n",
       "0    아가씨 담배한갑주소 네 4500원입니다 어 네 지갑어디갔지 에이 버스에서 잃어버렸나...   \n",
       "1    우리팀에서 다른팀으로 갈 사람 없나? 그럼 영지씨가 가는건 어때?  네? 제가요? ...   \n",
       "2    너 오늘 그게 뭐야 네 제가 뭘 잘못했나요.? 제대로 좀 하지 네 똑바로 좀 하지 ...   \n",
       "3    이거 들어바 와 이 노래 진짜 좋다 그치 요즘 이 것만 들어 진짜 너무 좋다 내가 ...   \n",
       "4    아무튼 앞으로 니가 내 와이파이야. .응 와이파이 온. 켰어. 반말? 주인님이라고도...   \n",
       "..                                                 ...   \n",
       "495  미나씨 휴가 결제 올리기 전에 저랑 상의하라고 말한거 기억해요? 네 합니다. 보고서...   \n",
       "496  교수님 제 논문에 제 이름이 없나요?  아 무슨 논문말이야?  지난 번 냈던 논문이...   \n",
       "497  야 너  네 저요? 그래 너 왜요 돈좀 줘봐  돈 없어요 돈이 왜 없어 지갑은 폼이...   \n",
       "498  야 너 빨리 안 뛰어와? 너 이 환자 제대로 봤어 안 봤어 어제 저녁부터 계속 보다...   \n",
       "499  엄마 저 그 돈 안해주시면 정말 큰일나요.  이유도 말하지 않고. 몇번째니 경민아....   \n",
       "\n",
       "                                          cleaned_text  \n",
       "0    아가씨 담배한갑주소 네 원입니다 어 네 지갑어디갔지 에이 버스에서 잃어버렸나보네 그...  \n",
       "1    우리팀에서 다른팀으로 갈 사람 없나 그럼 영지씨가 가는건 어때 네 제가요 그렇지 달...  \n",
       "2    너 오늘 그게 뭐야 네 제가 뭘 잘못했나요 제대로 좀 하지 네 똑바로 좀 하지 행실...  \n",
       "3    이거 들어바 와 이 노래 진짜 좋다 그치 요즘 이 것만 들어 진짜 너무 좋다 내가 ...  \n",
       "4    아무튼 앞으로 니가 내 와이파이야 응 와이파이 온 켰어 반말 주인님이라고도 말해야지...  \n",
       "..                                                 ...  \n",
       "495  미나씨 휴가 결제 올리기 전에 저랑 상의하라고 말한거 기억해요 네 합니다 보고서를 ...  \n",
       "496  교수님 제 논문에 제 이름이 없나요 아 무슨 논문말이야 지난 번 냈던 논문이요 그거...  \n",
       "497  야 너 네 저요 그래 너 왜요 돈좀 줘봐 돈 없어요 돈이 왜 없어 지갑은 폼이니 진...  \n",
       "498  야 너 빨리 안 뛰어와 너 이 환자 제대로 봤어 안 봤어 어제 저녁부터 계속 보다가...  \n",
       "499  엄마 저 그 돈 안해주시면 정말 큰일나요 이유도 말하지 않고 몇번째니 경민아 엄마 ...  \n",
       "\n",
       "[500 rows x 7 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_and_predict(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9131b908",
   "metadata": {},
   "source": [
    "### klue/bert-base - d_modelX2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9ff06860",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.tokenizer_type = 'pretrained' \n",
    "config.pretrained_model_name = \"klue/bert-base\"\n",
    "config.num_layers = 4\n",
    "config.d_model = 512\n",
    "config.ff_dim = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d04f294f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사전학습된 토크나이저 로드 중: klue/bert-base\n",
      "[Config Finalized] Vocab: 32000\n",
      " >> IDs - PAD:0, UNK:1, CLS:2, SEP:3, MASK:4\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(config, df[\"conversation\"].tolist())\n",
    "config.update_from_tokenizer(tokenizer) # Vocab Size 동기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b607fa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df[\"label\"])\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    ClassificationDataset(train_df[\"conversation\"].tolist(), train_df[\"label\"].tolist(), tokenizer, config, is_train=True),\n",
    "    batch_size=config.batch_size, shuffle=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    ClassificationDataset(val_df[\"conversation\"].tolist(), val_df[\"label\"].tolist(), tokenizer, config, is_train=False),\n",
    "    batch_size=config.batch_size\n",
    ")\n",
    "# debug(train_loader, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6d4c7e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): StandardBertModel(\n",
       "    (token_emb): Embedding(32000, 512)\n",
       "    (pos_emb): Embedding(128, 512)\n",
       "    (seg_emb): Embedding(2, 512)\n",
       "    (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x ModuleDict(\n",
       "        (mha): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.3, inplace=False)\n",
       "          (3): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (pooler): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (1): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (classifier): Linear(in_features=512, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. 모델 초기화\n",
    "backbone = StandardBertModel(config)\n",
    "model = BertForSequenceClassification(backbone, config, num_labels=5).to(device)\n",
    "model.apply(init_weights) # 가중치 안정화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3fe2c497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  [Train] Loss: 1.6105 | Acc: 0.1986\n",
      "  [Val]   Loss: 1.6081 | Acc: 0.2146 | F1: 0.0943\n",
      "  >> Best F1 Updated! (0.0943)\n",
      "  >> Best Loss Updated! (1.6081)\n",
      "  >> Best model Updated! (0.0943, 1.6081)\n",
      "--------------------------------------------------\n",
      "Epoch 2/100\n",
      "  [Train] Loss: 1.6092 | Acc: 0.2096\n",
      "  [Val]   Loss: 1.5968 | Acc: 0.2247 | F1: 0.1124\n",
      "  >> Best F1 Updated! (0.1124)\n",
      "  >> Best Loss Updated! (1.5968)\n",
      "  >> Best model Updated! (0.1124, 1.5968)\n",
      "--------------------------------------------------\n",
      "Epoch 3/100\n",
      "  [Train] Loss: 1.4423 | Acc: 0.4354\n",
      "  [Val]   Loss: 1.1774 | Acc: 0.6011 | F1: 0.5586\n",
      "  >> Best F1 Updated! (0.5586)\n",
      "  >> Best Loss Updated! (1.1774)\n",
      "  >> Best model Updated! (0.5586, 1.1774)\n",
      "--------------------------------------------------\n",
      "Epoch 4/100\n",
      "  [Train] Loss: 1.0630 | Acc: 0.6772\n",
      "  [Val]   Loss: 0.9525 | Acc: 0.7449 | F1: 0.7306\n",
      "  >> Best F1 Updated! (0.7306)\n",
      "  >> Best Loss Updated! (0.9525)\n",
      "  >> Best model Updated! (0.7306, 0.9525)\n",
      "--------------------------------------------------\n",
      "Epoch 5/100\n",
      "  [Train] Loss: 0.8827 | Acc: 0.7699\n",
      "  [Val]   Loss: 0.8443 | Acc: 0.7910 | F1: 0.7914\n",
      "  >> Best F1 Updated! (0.7914)\n",
      "  >> Best Loss Updated! (0.8443)\n",
      "  >> Best model Updated! (0.7914, 0.8443)\n",
      "--------------------------------------------------\n",
      "Epoch 6/100\n",
      "  [Train] Loss: 0.7431 | Acc: 0.8461\n",
      "  [Val]   Loss: 0.8086 | Acc: 0.8135 | F1: 0.8114\n",
      "  >> Best F1 Updated! (0.8114)\n",
      "  >> Best Loss Updated! (0.8086)\n",
      "  >> Best model Updated! (0.8114, 0.8086)\n",
      "--------------------------------------------------\n",
      "Epoch 7/100\n",
      "  [Train] Loss: 0.6505 | Acc: 0.8862\n",
      "  [Val]   Loss: 0.8046 | Acc: 0.8202 | F1: 0.8223\n",
      "  >> Best F1 Updated! (0.8223)\n",
      "  >> Best Loss Updated! (0.8046)\n",
      "  >> Best model Updated! (0.8223, 0.8046)\n",
      "--------------------------------------------------\n",
      "Epoch 8/100\n",
      "  [Train] Loss: 0.5678 | Acc: 0.9256\n",
      "  [Val]   Loss: 0.8004 | Acc: 0.8315 | F1: 0.8327\n",
      "  >> Best F1 Updated! (0.8327)\n",
      "  >> Best Loss Updated! (0.8004)\n",
      "  >> Best model Updated! (0.8327, 0.8004)\n",
      "--------------------------------------------------\n",
      "Epoch 9/100\n",
      "  [Train] Loss: 0.5241 | Acc: 0.9407\n",
      "  [Val]   Loss: 0.8536 | Acc: 0.8124 | F1: 0.8127\n",
      "  >> EarlyStopping counter: 1 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 10/100\n",
      "  [Train] Loss: 0.4943 | Acc: 0.9545\n",
      "  [Val]   Loss: 0.9355 | Acc: 0.7989 | F1: 0.7989\n",
      "  >> EarlyStopping counter: 2 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 11/100\n",
      "  [Train] Loss: 0.4738 | Acc: 0.9640\n",
      "  [Val]   Loss: 0.8511 | Acc: 0.8225 | F1: 0.8224\n",
      "  >> EarlyStopping counter: 3 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 12/100\n",
      "  [Train] Loss: 0.4545 | Acc: 0.9713\n",
      "  [Val]   Loss: 0.8622 | Acc: 0.8292 | F1: 0.8303\n",
      "  >> EarlyStopping counter: 4 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 13/100\n",
      "  [Train] Loss: 0.4521 | Acc: 0.9733\n",
      "  [Val]   Loss: 0.8954 | Acc: 0.8157 | F1: 0.8183\n",
      "  >> EarlyStopping counter: 5 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 14/100\n",
      "  [Train] Loss: 0.4382 | Acc: 0.9806\n",
      "  [Val]   Loss: 0.9315 | Acc: 0.8022 | F1: 0.8040\n",
      "  >> EarlyStopping counter: 6 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 15/100\n",
      "  [Train] Loss: 0.4339 | Acc: 0.9815\n",
      "  [Val]   Loss: 0.8980 | Acc: 0.8157 | F1: 0.8176\n",
      "  >> EarlyStopping counter: 7 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 16/100\n",
      "  [Train] Loss: 0.4218 | Acc: 0.9857\n",
      "  [Val]   Loss: 0.8965 | Acc: 0.8258 | F1: 0.8273\n",
      "  >> EarlyStopping counter: 8 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 17/100\n",
      "  [Train] Loss: 0.4186 | Acc: 0.9876\n",
      "  [Val]   Loss: 0.8994 | Acc: 0.8236 | F1: 0.8256\n",
      "  >> EarlyStopping counter: 9 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 18/100\n",
      "  [Train] Loss: 0.4260 | Acc: 0.9851\n",
      "  [Val]   Loss: 0.9425 | Acc: 0.8169 | F1: 0.8201\n",
      "  >> EarlyStopping counter: 10 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 19/100\n",
      "  [Train] Loss: 0.4138 | Acc: 0.9902\n",
      "  [Val]   Loss: 0.9488 | Acc: 0.8157 | F1: 0.8169\n",
      "  >> EarlyStopping counter: 11 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 20/100\n",
      "  [Train] Loss: 0.4164 | Acc: 0.9893\n",
      "  [Val]   Loss: 0.9681 | Acc: 0.8022 | F1: 0.8032\n",
      "  >> EarlyStopping counter: 12 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 21/100\n",
      "  [Train] Loss: 0.4142 | Acc: 0.9907\n",
      "  [Val]   Loss: 0.9508 | Acc: 0.8157 | F1: 0.8162\n",
      "  >> EarlyStopping counter: 13 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 22/100\n",
      "  [Train] Loss: 0.4226 | Acc: 0.9865\n",
      "  [Val]   Loss: 0.9958 | Acc: 0.8045 | F1: 0.8058\n",
      "  >> EarlyStopping counter: 14 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 23/100\n",
      "  [Train] Loss: 0.4100 | Acc: 0.9913\n",
      "  [Val]   Loss: 0.9610 | Acc: 0.8124 | F1: 0.8147\n",
      "  >> EarlyStopping counter: 15 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 24/100\n",
      "  [Train] Loss: 0.4166 | Acc: 0.9893\n",
      "  [Val]   Loss: 0.9657 | Acc: 0.8124 | F1: 0.8161\n",
      "  >> EarlyStopping counter: 16 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 25/100\n",
      "  [Train] Loss: 0.4109 | Acc: 0.9919\n",
      "  [Val]   Loss: 0.9389 | Acc: 0.8270 | F1: 0.8297\n",
      "  >> EarlyStopping counter: 17 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 26/100\n",
      "  [Train] Loss: 0.4103 | Acc: 0.9921\n",
      "  [Val]   Loss: 0.9604 | Acc: 0.8180 | F1: 0.8198\n",
      "  >> EarlyStopping counter: 18 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 27/100\n",
      "  [Train] Loss: 0.4122 | Acc: 0.9919\n",
      "  [Val]   Loss: 0.9501 | Acc: 0.8258 | F1: 0.8265\n",
      "  >> EarlyStopping counter: 19 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 28/100\n",
      "  [Train] Loss: 0.4106 | Acc: 0.9913\n",
      "  [Val]   Loss: 0.9502 | Acc: 0.8225 | F1: 0.8226\n",
      "  >> EarlyStopping counter: 20 out of 20\n",
      "성능 개선이 없어 28 에포크에서 학습을 조기 종료합니다.\n",
      "학습이 완료되었습니다. 최종 지표 분석을 시작합니다.\n",
      ">> 최고 성능 모델 복원 완료 (Epoch 8)\n",
      "val_acc:0.8314606741573034, val_acc: 0.8314606741573034, f1:0.8326638457327459\n",
      ">> Confusion Matrix image saved to ./results/klue_bert-base_d512_f1024_l4/final_best_confusion_matrix.png\n",
      ">> [Success] 시각화 결과가 저장되었습니다: ./results/klue_bert-base\n",
      ">> 모든 최종 결과물이 ./results/klue_bert-base 에 저장되었습니다.\n",
      "CPU times: user 4min 35s, sys: 18.6 s, total: 4min 53s\n",
      "Wall time: 4min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = train(\n",
    "   config, model, train_loader, val_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "942dd7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 가중치 로드 완료 (Epoch: 7, F1: 0.8327)\n",
      "모델 가중치 로드 완료 (Epoch: 7, F1: 0.8327)\n",
      "사전학습된 토크나이저 로드 중: klue/bert-base\n",
      "추론 시작... (테스트 데이터: 500개)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 16/16 [00:00<00:00, 33.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최종 결과 저장 완료: ./dataset/klue_bert-base_d512_f1024_l4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>class</th>\n",
       "      <th>text_label</th>\n",
       "      <th>confidence</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>conversation</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t_000</td>\n",
       "      <td>1</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>0.9021</td>\n",
       "      <td>아가씨 담배 ##한 ##갑 ##주 ##소 네 원 ##입니다 어 네 지갑 ##어 ##...</td>\n",
       "      <td>아가씨 담배한갑주소 네 4500원입니다 어 네 지갑어디갔지 에이 버스에서 잃어버렸나...</td>\n",
       "      <td>아가씨 담배한갑주소 네 원입니다 어 네 지갑어디갔지 에이 버스에서 잃어버렸나보네 그...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t_001</td>\n",
       "      <td>2</td>\n",
       "      <td>직장 내 괴롭힘 대화</td>\n",
       "      <td>0.9300</td>\n",
       "      <td>우리 ##팀 ##에서 다른 ##팀 ##으로 갈 사람 없 ##나 그럼 영지 ##씨 #...</td>\n",
       "      <td>우리팀에서 다른팀으로 갈 사람 없나? 그럼 영지씨가 가는건 어때?&nbsp;&nbsp;네? 제가요? ...</td>\n",
       "      <td>우리팀에서 다른팀으로 갈 사람 없나 그럼 영지씨가 가는건 어때 네 제가요 그렇지 달...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t_002</td>\n",
       "      <td>2</td>\n",
       "      <td>직장 내 괴롭힘 대화</td>\n",
       "      <td>0.8324</td>\n",
       "      <td>너 오늘 그게 뭐 ##야 네 제 ##가 뭘 잘못 ##했 ##나 ##요 제대로 좀 하...</td>\n",
       "      <td>너 오늘 그게 뭐야 네 제가 뭘 잘못했나요.? 제대로 좀 하지 네 똑바로 좀 하지 ...</td>\n",
       "      <td>너 오늘 그게 뭐야 네 제가 뭘 잘못했나요 제대로 좀 하지 네 똑바로 좀 하지 행실...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t_003</td>\n",
       "      <td>4</td>\n",
       "      <td>일반 대화</td>\n",
       "      <td>0.8820</td>\n",
       "      <td>이거 들어 ##바 와 이 노래 진짜 좋 ##다 그치 요즘 이 것 ##만 들어 진짜 ...</td>\n",
       "      <td>이거 들어바 와 이 노래 진짜 좋다 그치 요즘 이 것만 들어 진짜 너무 좋다 내가 ...</td>\n",
       "      <td>이거 들어바 와 이 노래 진짜 좋다 그치 요즘 이 것만 들어 진짜 너무 좋다 내가 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t_004</td>\n",
       "      <td>3</td>\n",
       "      <td>기타 괴롭힘 대화</td>\n",
       "      <td>0.9348</td>\n",
       "      <td>아무튼 앞 ##으로 니 ##가 내 와이파이 ##야 응 와이파이 온 켰 ##어 반말 ...</td>\n",
       "      <td>아무튼 앞으로 니가 내 와이파이야. .응 와이파이 온. 켰어. 반말? 주인님이라고도...</td>\n",
       "      <td>아무튼 앞으로 니가 내 와이파이야 응 와이파이 온 켰어 반말 주인님이라고도 말해야지...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>t_495</td>\n",
       "      <td>2</td>\n",
       "      <td>직장 내 괴롭힘 대화</td>\n",
       "      <td>0.9260</td>\n",
       "      <td>미나 ##씨 휴가 결제 올리 ##기 전 ##에 저 ##랑 상의 ##하라 ##고 말 ...</td>\n",
       "      <td>미나씨 휴가 결제 올리기 전에 저랑 상의하라고 말한거 기억해요? 네 합니다. 보고서...</td>\n",
       "      <td>미나씨 휴가 결제 올리기 전에 저랑 상의하라고 말한거 기억해요 네 합니다 보고서를 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>t_496</td>\n",
       "      <td>3</td>\n",
       "      <td>기타 괴롭힘 대화</td>\n",
       "      <td>0.9289</td>\n",
       "      <td>교수 ##님 제 논문 ##에 제 이름 ##이 없 ##나 ##요 아 무슨 논문 ##말...</td>\n",
       "      <td>교수님 제 논문에 제 이름이 없나요?&nbsp;&nbsp;아 무슨 논문말이야?&nbsp;&nbsp;지난 번 냈던 논문이...</td>\n",
       "      <td>교수님 제 논문에 제 이름이 없나요 아 무슨 논문말이야 지난 번 냈던 논문이요 그거...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>t_497</td>\n",
       "      <td>1</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>0.9241</td>\n",
       "      <td>야 너 네 저 ##요 그래 너 왜 ##요 돈 ##좀 줘 ##봐 돈 없 ##어요 돈 ...</td>\n",
       "      <td>야 너&nbsp;&nbsp;네 저요? 그래 너 왜요 돈좀 줘봐&nbsp;&nbsp;돈 없어요 돈이 왜 없어 지갑은 폼이...</td>\n",
       "      <td>야 너 네 저요 그래 너 왜요 돈좀 줘봐 돈 없어요 돈이 왜 없어 지갑은 폼이니 진...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>t_498</td>\n",
       "      <td>2</td>\n",
       "      <td>직장 내 괴롭힘 대화</td>\n",
       "      <td>0.8483</td>\n",
       "      <td>야 너 빨리 안 뛰어 ##와 너 이 환자 제대로 봤 ##어 안 봤 ##어 어제 저녁...</td>\n",
       "      <td>야 너 빨리 안 뛰어와? 너 이 환자 제대로 봤어 안 봤어 어제 저녁부터 계속 보다...</td>\n",
       "      <td>야 너 빨리 안 뛰어와 너 이 환자 제대로 봤어 안 봤어 어제 저녁부터 계속 보다가...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>t_499</td>\n",
       "      <td>0</td>\n",
       "      <td>협박 대화</td>\n",
       "      <td>0.8710</td>\n",
       "      <td>엄마 저 그 돈 안 ##해 ##주 ##시 ##면 정말 큰일 ##나 ##요 이유 ##...</td>\n",
       "      <td>엄마 저 그 돈 안해주시면 정말 큰일나요.&nbsp;&nbsp;이유도 말하지 않고. 몇번째니 경민아....</td>\n",
       "      <td>엄마 저 그 돈 안해주시면 정말 큰일나요 이유도 말하지 않고 몇번째니 경민아 엄마 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       idx  class   text_label  confidence  \\\n",
       "0    t_000      1        갈취 대화      0.9021   \n",
       "1    t_001      2  직장 내 괴롭힘 대화      0.9300   \n",
       "2    t_002      2  직장 내 괴롭힘 대화      0.8324   \n",
       "3    t_003      4        일반 대화      0.8820   \n",
       "4    t_004      3    기타 괴롭힘 대화      0.9348   \n",
       "..     ...    ...          ...         ...   \n",
       "495  t_495      2  직장 내 괴롭힘 대화      0.9260   \n",
       "496  t_496      3    기타 괴롭힘 대화      0.9289   \n",
       "497  t_497      1        갈취 대화      0.9241   \n",
       "498  t_498      2  직장 내 괴롭힘 대화      0.8483   \n",
       "499  t_499      0        협박 대화      0.8710   \n",
       "\n",
       "                                        tokenized_text  \\\n",
       "0    아가씨 담배 ##한 ##갑 ##주 ##소 네 원 ##입니다 어 네 지갑 ##어 ##...   \n",
       "1    우리 ##팀 ##에서 다른 ##팀 ##으로 갈 사람 없 ##나 그럼 영지 ##씨 #...   \n",
       "2    너 오늘 그게 뭐 ##야 네 제 ##가 뭘 잘못 ##했 ##나 ##요 제대로 좀 하...   \n",
       "3    이거 들어 ##바 와 이 노래 진짜 좋 ##다 그치 요즘 이 것 ##만 들어 진짜 ...   \n",
       "4    아무튼 앞 ##으로 니 ##가 내 와이파이 ##야 응 와이파이 온 켰 ##어 반말 ...   \n",
       "..                                                 ...   \n",
       "495  미나 ##씨 휴가 결제 올리 ##기 전 ##에 저 ##랑 상의 ##하라 ##고 말 ...   \n",
       "496  교수 ##님 제 논문 ##에 제 이름 ##이 없 ##나 ##요 아 무슨 논문 ##말...   \n",
       "497  야 너 네 저 ##요 그래 너 왜 ##요 돈 ##좀 줘 ##봐 돈 없 ##어요 돈 ...   \n",
       "498  야 너 빨리 안 뛰어 ##와 너 이 환자 제대로 봤 ##어 안 봤 ##어 어제 저녁...   \n",
       "499  엄마 저 그 돈 안 ##해 ##주 ##시 ##면 정말 큰일 ##나 ##요 이유 ##...   \n",
       "\n",
       "                                          conversation  \\\n",
       "0    아가씨 담배한갑주소 네 4500원입니다 어 네 지갑어디갔지 에이 버스에서 잃어버렸나...   \n",
       "1    우리팀에서 다른팀으로 갈 사람 없나? 그럼 영지씨가 가는건 어때?  네? 제가요? ...   \n",
       "2    너 오늘 그게 뭐야 네 제가 뭘 잘못했나요.? 제대로 좀 하지 네 똑바로 좀 하지 ...   \n",
       "3    이거 들어바 와 이 노래 진짜 좋다 그치 요즘 이 것만 들어 진짜 너무 좋다 내가 ...   \n",
       "4    아무튼 앞으로 니가 내 와이파이야. .응 와이파이 온. 켰어. 반말? 주인님이라고도...   \n",
       "..                                                 ...   \n",
       "495  미나씨 휴가 결제 올리기 전에 저랑 상의하라고 말한거 기억해요? 네 합니다. 보고서...   \n",
       "496  교수님 제 논문에 제 이름이 없나요?  아 무슨 논문말이야?  지난 번 냈던 논문이...   \n",
       "497  야 너  네 저요? 그래 너 왜요 돈좀 줘봐  돈 없어요 돈이 왜 없어 지갑은 폼이...   \n",
       "498  야 너 빨리 안 뛰어와? 너 이 환자 제대로 봤어 안 봤어 어제 저녁부터 계속 보다...   \n",
       "499  엄마 저 그 돈 안해주시면 정말 큰일나요.  이유도 말하지 않고. 몇번째니 경민아....   \n",
       "\n",
       "                                          cleaned_text  \n",
       "0    아가씨 담배한갑주소 네 원입니다 어 네 지갑어디갔지 에이 버스에서 잃어버렸나보네 그...  \n",
       "1    우리팀에서 다른팀으로 갈 사람 없나 그럼 영지씨가 가는건 어때 네 제가요 그렇지 달...  \n",
       "2    너 오늘 그게 뭐야 네 제가 뭘 잘못했나요 제대로 좀 하지 네 똑바로 좀 하지 행실...  \n",
       "3    이거 들어바 와 이 노래 진짜 좋다 그치 요즘 이 것만 들어 진짜 너무 좋다 내가 ...  \n",
       "4    아무튼 앞으로 니가 내 와이파이야 응 와이파이 온 켰어 반말 주인님이라고도 말해야지...  \n",
       "..                                                 ...  \n",
       "495  미나씨 휴가 결제 올리기 전에 저랑 상의하라고 말한거 기억해요 네 합니다 보고서를 ...  \n",
       "496  교수님 제 논문에 제 이름이 없나요 아 무슨 논문말이야 지난 번 냈던 논문이요 그거...  \n",
       "497  야 너 네 저요 그래 너 왜요 돈좀 줘봐 돈 없어요 돈이 왜 없어 지갑은 폼이니 진...  \n",
       "498  야 너 빨리 안 뛰어와 너 이 환자 제대로 봤어 안 봤어 어제 저녁부터 계속 보다가...  \n",
       "499  엄마 저 그 돈 안해주시면 정말 큰일나요 이유도 말하지 않고 몇번째니 경민아 엄마 ...  \n",
       "\n",
       "[500 rows x 7 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_and_predict(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3f9619",
   "metadata": {},
   "source": [
    "### klue/bert-base - ff_dim*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2eab791a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.tokenizer_type = 'pretrained' \n",
    "config.pretrained_model_name = \"klue/bert-base\"\n",
    "config.num_layers = 4\n",
    "config.d_model = 256\n",
    "config.ff_dim = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "66b53943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사전학습된 토크나이저 로드 중: klue/bert-base\n",
      "[Config Finalized] Vocab: 32000\n",
      " >> IDs - PAD:0, UNK:1, CLS:2, SEP:3, MASK:4\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(config, df[\"conversation\"].tolist())\n",
    "config.update_from_tokenizer(tokenizer) # Vocab Size 동기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "62a39885",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df[\"label\"])\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    ClassificationDataset(train_df[\"conversation\"].tolist(), train_df[\"label\"].tolist(), tokenizer, config, is_train=True),\n",
    "    batch_size=config.batch_size, shuffle=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    ClassificationDataset(val_df[\"conversation\"].tolist(), val_df[\"label\"].tolist(), tokenizer, config, is_train=False),\n",
    "    batch_size=config.batch_size\n",
    ")\n",
    "# debug(train_loader, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9113296f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): StandardBertModel(\n",
       "    (token_emb): Embedding(32000, 256)\n",
       "    (pos_emb): Embedding(128, 256)\n",
       "    (seg_emb): Embedding(2, 256)\n",
       "    (norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x ModuleDict(\n",
       "        (mha): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.3, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (4): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (pooler): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (1): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (classifier): Linear(in_features=256, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. 모델 초기화\n",
    "backbone = StandardBertModel(config)\n",
    "model = BertForSequenceClassification(backbone, config, num_labels=5).to(device)\n",
    "model.apply(init_weights) # 가중치 안정화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c0d108b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  [Train] Loss: 1.6103 | Acc: 0.1890\n",
      "  [Val]   Loss: 1.6096 | Acc: 0.2000 | F1: 0.0667\n",
      "  >> Best F1 Updated! (0.0667)\n",
      "  >> Best Loss Updated! (1.6096)\n",
      "  >> Best model Updated! (0.0667, 1.6096)\n",
      "--------------------------------------------------\n",
      "Epoch 2/100\n",
      "  [Train] Loss: 1.6094 | Acc: 0.1997\n",
      "  [Val]   Loss: 1.6084 | Acc: 0.2022 | F1: 0.0716\n",
      "  >> Best F1 Updated! (0.0716)\n",
      "  >> Best Loss Updated! (1.6084)\n",
      "  >> Best model Updated! (0.0716, 1.6084)\n",
      "--------------------------------------------------\n",
      "Epoch 3/100\n",
      "  [Train] Loss: 1.6079 | Acc: 0.2115\n",
      "  [Val]   Loss: 1.5983 | Acc: 0.2191 | F1: 0.1042\n",
      "  >> Best F1 Updated! (0.1042)\n",
      "  >> Best Loss Updated! (1.5983)\n",
      "  >> Best model Updated! (0.1042, 1.5983)\n",
      "--------------------------------------------------\n",
      "Epoch 4/100\n",
      "  [Train] Loss: 1.4268 | Acc: 0.4500\n",
      "  [Val]   Loss: 1.2222 | Acc: 0.5787 | F1: 0.5161\n",
      "  >> Best F1 Updated! (0.5161)\n",
      "  >> Best Loss Updated! (1.2222)\n",
      "  >> Best model Updated! (0.5161, 1.2222)\n",
      "--------------------------------------------------\n",
      "Epoch 5/100\n",
      "  [Train] Loss: 1.1331 | Acc: 0.6188\n",
      "  [Val]   Loss: 1.0372 | Acc: 0.6955 | F1: 0.6721\n",
      "  >> Best F1 Updated! (0.6721)\n",
      "  >> Best Loss Updated! (1.0372)\n",
      "  >> Best model Updated! (0.6721, 1.0372)\n",
      "--------------------------------------------------\n",
      "Epoch 6/100\n",
      "  [Train] Loss: 0.9332 | Acc: 0.7587\n",
      "  [Val]   Loss: 0.8741 | Acc: 0.7798 | F1: 0.7738\n",
      "  >> Best F1 Updated! (0.7738)\n",
      "  >> Best Loss Updated! (0.8741)\n",
      "  >> Best model Updated! (0.7738, 0.8741)\n",
      "--------------------------------------------------\n",
      "Epoch 7/100\n",
      "  [Train] Loss: 0.7749 | Acc: 0.8430\n",
      "  [Val]   Loss: 0.8199 | Acc: 0.7989 | F1: 0.7961\n",
      "  >> Best F1 Updated! (0.7961)\n",
      "  >> Best Loss Updated! (0.8199)\n",
      "  >> Best model Updated! (0.7961, 0.8199)\n",
      "--------------------------------------------------\n",
      "Epoch 8/100\n",
      "  [Train] Loss: 0.6770 | Acc: 0.8787\n",
      "  [Val]   Loss: 0.8134 | Acc: 0.8034 | F1: 0.8052\n",
      "  >> Best F1 Updated! (0.8052)\n",
      "  >> Best Loss Updated! (0.8134)\n",
      "  >> Best model Updated! (0.8052, 0.8134)\n",
      "--------------------------------------------------\n",
      "Epoch 9/100\n",
      "  [Train] Loss: 0.6151 | Acc: 0.9135\n",
      "  [Val]   Loss: 0.7834 | Acc: 0.8225 | F1: 0.8227\n",
      "  >> Best F1 Updated! (0.8227)\n",
      "  >> Best Loss Updated! (0.7834)\n",
      "  >> Best model Updated! (0.8227, 0.7834)\n",
      "--------------------------------------------------\n",
      "Epoch 10/100\n",
      "  [Train] Loss: 0.5647 | Acc: 0.9362\n",
      "  [Val]   Loss: 0.7975 | Acc: 0.8258 | F1: 0.8266\n",
      "  >> Best F1 Updated! (0.8266)\n",
      "  >> Best model Updated! (0.8266, 0.7975)\n",
      "--------------------------------------------------\n",
      "Epoch 11/100\n",
      "  [Train] Loss: 0.5281 | Acc: 0.9508\n",
      "  [Val]   Loss: 0.8104 | Acc: 0.8213 | F1: 0.8219\n",
      "  >> EarlyStopping counter: 1 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 12/100\n",
      "  [Train] Loss: 0.5094 | Acc: 0.9584\n",
      "  [Val]   Loss: 0.8440 | Acc: 0.8124 | F1: 0.8140\n",
      "  >> EarlyStopping counter: 2 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 13/100\n",
      "  [Train] Loss: 0.4865 | Acc: 0.9683\n",
      "  [Val]   Loss: 0.8364 | Acc: 0.8135 | F1: 0.8165\n",
      "  >> EarlyStopping counter: 3 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 14/100\n",
      "  [Train] Loss: 0.4619 | Acc: 0.9719\n",
      "  [Val]   Loss: 0.8698 | Acc: 0.8157 | F1: 0.8166\n",
      "  >> EarlyStopping counter: 4 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 15/100\n",
      "  [Train] Loss: 0.4459 | Acc: 0.9764\n",
      "  [Val]   Loss: 0.8857 | Acc: 0.8191 | F1: 0.8197\n",
      "  >> EarlyStopping counter: 5 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 16/100\n",
      "  [Train] Loss: 0.4382 | Acc: 0.9789\n",
      "  [Val]   Loss: 0.9184 | Acc: 0.8101 | F1: 0.8135\n",
      "  >> EarlyStopping counter: 6 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 17/100\n",
      "  [Train] Loss: 0.4383 | Acc: 0.9803\n",
      "  [Val]   Loss: 0.9269 | Acc: 0.8067 | F1: 0.8093\n",
      "  >> EarlyStopping counter: 7 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 18/100\n",
      "  [Train] Loss: 0.4369 | Acc: 0.9817\n",
      "  [Val]   Loss: 0.9800 | Acc: 0.7921 | F1: 0.7912\n",
      "  >> EarlyStopping counter: 8 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 19/100\n",
      "  [Train] Loss: 0.4357 | Acc: 0.9798\n",
      "  [Val]   Loss: 0.9557 | Acc: 0.8101 | F1: 0.8131\n",
      "  >> EarlyStopping counter: 9 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 20/100\n",
      "  [Train] Loss: 0.4312 | Acc: 0.9823\n",
      "  [Val]   Loss: 1.0093 | Acc: 0.7944 | F1: 0.7970\n",
      "  >> EarlyStopping counter: 10 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 21/100\n",
      "  [Train] Loss: 0.4293 | Acc: 0.9837\n",
      "  [Val]   Loss: 0.9414 | Acc: 0.8135 | F1: 0.8142\n",
      "  >> EarlyStopping counter: 11 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 22/100\n",
      "  [Train] Loss: 0.4207 | Acc: 0.9868\n",
      "  [Val]   Loss: 0.9608 | Acc: 0.8000 | F1: 0.8019\n",
      "  >> EarlyStopping counter: 12 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 23/100\n",
      "  [Train] Loss: 0.4213 | Acc: 0.9857\n",
      "  [Val]   Loss: 0.9653 | Acc: 0.8135 | F1: 0.8163\n",
      "  >> EarlyStopping counter: 13 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 24/100\n",
      "  [Train] Loss: 0.4150 | Acc: 0.9902\n",
      "  [Val]   Loss: 0.9765 | Acc: 0.8101 | F1: 0.8100\n",
      "  >> EarlyStopping counter: 14 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 25/100\n",
      "  [Train] Loss: 0.4228 | Acc: 0.9857\n",
      "  [Val]   Loss: 0.9832 | Acc: 0.8011 | F1: 0.8042\n",
      "  >> EarlyStopping counter: 15 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 26/100\n",
      "  [Train] Loss: 0.4240 | Acc: 0.9862\n",
      "  [Val]   Loss: 0.9961 | Acc: 0.8011 | F1: 0.8060\n",
      "  >> EarlyStopping counter: 16 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 27/100\n",
      "  [Train] Loss: 0.4115 | Acc: 0.9910\n",
      "  [Val]   Loss: 0.9576 | Acc: 0.8146 | F1: 0.8168\n",
      "  >> EarlyStopping counter: 17 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 28/100\n",
      "  [Train] Loss: 0.4199 | Acc: 0.9896\n",
      "  [Val]   Loss: 0.9933 | Acc: 0.8056 | F1: 0.8099\n",
      "  >> EarlyStopping counter: 18 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 29/100\n",
      "  [Train] Loss: 0.4104 | Acc: 0.9916\n",
      "  [Val]   Loss: 0.9807 | Acc: 0.8135 | F1: 0.8182\n",
      "  >> EarlyStopping counter: 19 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 30/100\n",
      "  [Train] Loss: 0.4138 | Acc: 0.9899\n",
      "  [Val]   Loss: 0.9924 | Acc: 0.8045 | F1: 0.8073\n",
      "  >> EarlyStopping counter: 20 out of 20\n",
      "성능 개선이 없어 30 에포크에서 학습을 조기 종료합니다.\n",
      "학습이 완료되었습니다. 최종 지표 분석을 시작합니다.\n",
      ">> 최고 성능 모델 복원 완료 (Epoch 10)\n",
      "val_acc:0.8258426966292135, val_acc: 0.8258426966292135, f1:0.8266047998731982\n",
      ">> Confusion Matrix image saved to ./results/klue_bert-base_d256_f2048_l4/final_best_confusion_matrix.png\n",
      ">> [Success] 시각화 결과가 저장되었습니다: ./results/klue_bert-base\n",
      ">> 모든 최종 결과물이 ./results/klue_bert-base 에 저장되었습니다.\n",
      "CPU times: user 3min 33s, sys: 17.8 s, total: 3min 50s\n",
      "Wall time: 3min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = train(\n",
    "   config, model, train_loader, val_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3ec70dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 가중치 로드 완료 (Epoch: 9, F1: 0.8266)\n",
      "모델 가중치 로드 완료 (Epoch: 9, F1: 0.8266)\n",
      "사전학습된 토크나이저 로드 중: klue/bert-base\n",
      "추론 시작... (테스트 데이터: 500개)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 16/16 [00:00<00:00, 36.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최종 결과 저장 완료: ./dataset/klue_bert-base_d256_f2048_l4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>class</th>\n",
       "      <th>text_label</th>\n",
       "      <th>confidence</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>conversation</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t_000</td>\n",
       "      <td>1</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>0.8020</td>\n",
       "      <td>아가씨 담배 ##한 ##갑 ##주 ##소 네 원 ##입니다 어 네 지갑 ##어 ##...</td>\n",
       "      <td>아가씨 담배한갑주소 네 4500원입니다 어 네 지갑어디갔지 에이 버스에서 잃어버렸나...</td>\n",
       "      <td>아가씨 담배한갑주소 네 원입니다 어 네 지갑어디갔지 에이 버스에서 잃어버렸나보네 그...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t_001</td>\n",
       "      <td>2</td>\n",
       "      <td>직장 내 괴롭힘 대화</td>\n",
       "      <td>0.9159</td>\n",
       "      <td>우리 ##팀 ##에서 다른 ##팀 ##으로 갈 사람 없 ##나 그럼 영지 ##씨 #...</td>\n",
       "      <td>우리팀에서 다른팀으로 갈 사람 없나? 그럼 영지씨가 가는건 어때?&nbsp;&nbsp;네? 제가요? ...</td>\n",
       "      <td>우리팀에서 다른팀으로 갈 사람 없나 그럼 영지씨가 가는건 어때 네 제가요 그렇지 달...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t_002</td>\n",
       "      <td>2</td>\n",
       "      <td>직장 내 괴롭힘 대화</td>\n",
       "      <td>0.9139</td>\n",
       "      <td>너 오늘 그게 뭐 ##야 네 제 ##가 뭘 잘못 ##했 ##나 ##요 제대로 좀 하...</td>\n",
       "      <td>너 오늘 그게 뭐야 네 제가 뭘 잘못했나요.? 제대로 좀 하지 네 똑바로 좀 하지 ...</td>\n",
       "      <td>너 오늘 그게 뭐야 네 제가 뭘 잘못했나요 제대로 좀 하지 네 똑바로 좀 하지 행실...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t_003</td>\n",
       "      <td>4</td>\n",
       "      <td>일반 대화</td>\n",
       "      <td>0.8760</td>\n",
       "      <td>이거 들어 ##바 와 이 노래 진짜 좋 ##다 그치 요즘 이 것 ##만 들어 진짜 ...</td>\n",
       "      <td>이거 들어바 와 이 노래 진짜 좋다 그치 요즘 이 것만 들어 진짜 너무 좋다 내가 ...</td>\n",
       "      <td>이거 들어바 와 이 노래 진짜 좋다 그치 요즘 이 것만 들어 진짜 너무 좋다 내가 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t_004</td>\n",
       "      <td>3</td>\n",
       "      <td>기타 괴롭힘 대화</td>\n",
       "      <td>0.8683</td>\n",
       "      <td>아무튼 앞 ##으로 니 ##가 내 와이파이 ##야 응 와이파이 온 켰 ##어 반말 ...</td>\n",
       "      <td>아무튼 앞으로 니가 내 와이파이야. .응 와이파이 온. 켰어. 반말? 주인님이라고도...</td>\n",
       "      <td>아무튼 앞으로 니가 내 와이파이야 응 와이파이 온 켰어 반말 주인님이라고도 말해야지...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>t_495</td>\n",
       "      <td>2</td>\n",
       "      <td>직장 내 괴롭힘 대화</td>\n",
       "      <td>0.9102</td>\n",
       "      <td>미나 ##씨 휴가 결제 올리 ##기 전 ##에 저 ##랑 상의 ##하라 ##고 말 ...</td>\n",
       "      <td>미나씨 휴가 결제 올리기 전에 저랑 상의하라고 말한거 기억해요? 네 합니다. 보고서...</td>\n",
       "      <td>미나씨 휴가 결제 올리기 전에 저랑 상의하라고 말한거 기억해요 네 합니다 보고서를 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>t_496</td>\n",
       "      <td>0</td>\n",
       "      <td>협박 대화</td>\n",
       "      <td>0.6954</td>\n",
       "      <td>교수 ##님 제 논문 ##에 제 이름 ##이 없 ##나 ##요 아 무슨 논문 ##말...</td>\n",
       "      <td>교수님 제 논문에 제 이름이 없나요?&nbsp;&nbsp;아 무슨 논문말이야?&nbsp;&nbsp;지난 번 냈던 논문이...</td>\n",
       "      <td>교수님 제 논문에 제 이름이 없나요 아 무슨 논문말이야 지난 번 냈던 논문이요 그거...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>t_497</td>\n",
       "      <td>1</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>0.8662</td>\n",
       "      <td>야 너 네 저 ##요 그래 너 왜 ##요 돈 ##좀 줘 ##봐 돈 없 ##어요 돈 ...</td>\n",
       "      <td>야 너&nbsp;&nbsp;네 저요? 그래 너 왜요 돈좀 줘봐&nbsp;&nbsp;돈 없어요 돈이 왜 없어 지갑은 폼이...</td>\n",
       "      <td>야 너 네 저요 그래 너 왜요 돈좀 줘봐 돈 없어요 돈이 왜 없어 지갑은 폼이니 진...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>t_498</td>\n",
       "      <td>2</td>\n",
       "      <td>직장 내 괴롭힘 대화</td>\n",
       "      <td>0.6819</td>\n",
       "      <td>야 너 빨리 안 뛰어 ##와 너 이 환자 제대로 봤 ##어 안 봤 ##어 어제 저녁...</td>\n",
       "      <td>야 너 빨리 안 뛰어와? 너 이 환자 제대로 봤어 안 봤어 어제 저녁부터 계속 보다...</td>\n",
       "      <td>야 너 빨리 안 뛰어와 너 이 환자 제대로 봤어 안 봤어 어제 저녁부터 계속 보다가...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>t_499</td>\n",
       "      <td>0</td>\n",
       "      <td>협박 대화</td>\n",
       "      <td>0.5469</td>\n",
       "      <td>엄마 저 그 돈 안 ##해 ##주 ##시 ##면 정말 큰일 ##나 ##요 이유 ##...</td>\n",
       "      <td>엄마 저 그 돈 안해주시면 정말 큰일나요.&nbsp;&nbsp;이유도 말하지 않고. 몇번째니 경민아....</td>\n",
       "      <td>엄마 저 그 돈 안해주시면 정말 큰일나요 이유도 말하지 않고 몇번째니 경민아 엄마 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       idx  class   text_label  confidence  \\\n",
       "0    t_000      1        갈취 대화      0.8020   \n",
       "1    t_001      2  직장 내 괴롭힘 대화      0.9159   \n",
       "2    t_002      2  직장 내 괴롭힘 대화      0.9139   \n",
       "3    t_003      4        일반 대화      0.8760   \n",
       "4    t_004      3    기타 괴롭힘 대화      0.8683   \n",
       "..     ...    ...          ...         ...   \n",
       "495  t_495      2  직장 내 괴롭힘 대화      0.9102   \n",
       "496  t_496      0        협박 대화      0.6954   \n",
       "497  t_497      1        갈취 대화      0.8662   \n",
       "498  t_498      2  직장 내 괴롭힘 대화      0.6819   \n",
       "499  t_499      0        협박 대화      0.5469   \n",
       "\n",
       "                                        tokenized_text  \\\n",
       "0    아가씨 담배 ##한 ##갑 ##주 ##소 네 원 ##입니다 어 네 지갑 ##어 ##...   \n",
       "1    우리 ##팀 ##에서 다른 ##팀 ##으로 갈 사람 없 ##나 그럼 영지 ##씨 #...   \n",
       "2    너 오늘 그게 뭐 ##야 네 제 ##가 뭘 잘못 ##했 ##나 ##요 제대로 좀 하...   \n",
       "3    이거 들어 ##바 와 이 노래 진짜 좋 ##다 그치 요즘 이 것 ##만 들어 진짜 ...   \n",
       "4    아무튼 앞 ##으로 니 ##가 내 와이파이 ##야 응 와이파이 온 켰 ##어 반말 ...   \n",
       "..                                                 ...   \n",
       "495  미나 ##씨 휴가 결제 올리 ##기 전 ##에 저 ##랑 상의 ##하라 ##고 말 ...   \n",
       "496  교수 ##님 제 논문 ##에 제 이름 ##이 없 ##나 ##요 아 무슨 논문 ##말...   \n",
       "497  야 너 네 저 ##요 그래 너 왜 ##요 돈 ##좀 줘 ##봐 돈 없 ##어요 돈 ...   \n",
       "498  야 너 빨리 안 뛰어 ##와 너 이 환자 제대로 봤 ##어 안 봤 ##어 어제 저녁...   \n",
       "499  엄마 저 그 돈 안 ##해 ##주 ##시 ##면 정말 큰일 ##나 ##요 이유 ##...   \n",
       "\n",
       "                                          conversation  \\\n",
       "0    아가씨 담배한갑주소 네 4500원입니다 어 네 지갑어디갔지 에이 버스에서 잃어버렸나...   \n",
       "1    우리팀에서 다른팀으로 갈 사람 없나? 그럼 영지씨가 가는건 어때?  네? 제가요? ...   \n",
       "2    너 오늘 그게 뭐야 네 제가 뭘 잘못했나요.? 제대로 좀 하지 네 똑바로 좀 하지 ...   \n",
       "3    이거 들어바 와 이 노래 진짜 좋다 그치 요즘 이 것만 들어 진짜 너무 좋다 내가 ...   \n",
       "4    아무튼 앞으로 니가 내 와이파이야. .응 와이파이 온. 켰어. 반말? 주인님이라고도...   \n",
       "..                                                 ...   \n",
       "495  미나씨 휴가 결제 올리기 전에 저랑 상의하라고 말한거 기억해요? 네 합니다. 보고서...   \n",
       "496  교수님 제 논문에 제 이름이 없나요?  아 무슨 논문말이야?  지난 번 냈던 논문이...   \n",
       "497  야 너  네 저요? 그래 너 왜요 돈좀 줘봐  돈 없어요 돈이 왜 없어 지갑은 폼이...   \n",
       "498  야 너 빨리 안 뛰어와? 너 이 환자 제대로 봤어 안 봤어 어제 저녁부터 계속 보다...   \n",
       "499  엄마 저 그 돈 안해주시면 정말 큰일나요.  이유도 말하지 않고. 몇번째니 경민아....   \n",
       "\n",
       "                                          cleaned_text  \n",
       "0    아가씨 담배한갑주소 네 원입니다 어 네 지갑어디갔지 에이 버스에서 잃어버렸나보네 그...  \n",
       "1    우리팀에서 다른팀으로 갈 사람 없나 그럼 영지씨가 가는건 어때 네 제가요 그렇지 달...  \n",
       "2    너 오늘 그게 뭐야 네 제가 뭘 잘못했나요 제대로 좀 하지 네 똑바로 좀 하지 행실...  \n",
       "3    이거 들어바 와 이 노래 진짜 좋다 그치 요즘 이 것만 들어 진짜 너무 좋다 내가 ...  \n",
       "4    아무튼 앞으로 니가 내 와이파이야 응 와이파이 온 켰어 반말 주인님이라고도 말해야지...  \n",
       "..                                                 ...  \n",
       "495  미나씨 휴가 결제 올리기 전에 저랑 상의하라고 말한거 기억해요 네 합니다 보고서를 ...  \n",
       "496  교수님 제 논문에 제 이름이 없나요 아 무슨 논문말이야 지난 번 냈던 논문이요 그거...  \n",
       "497  야 너 네 저요 그래 너 왜요 돈좀 줘봐 돈 없어요 돈이 왜 없어 지갑은 폼이니 진...  \n",
       "498  야 너 빨리 안 뛰어와 너 이 환자 제대로 봤어 안 봤어 어제 저녁부터 계속 보다가...  \n",
       "499  엄마 저 그 돈 안해주시면 정말 큰일나요 이유도 말하지 않고 몇번째니 경민아 엄마 ...  \n",
       "\n",
       "[500 rows x 7 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_and_predict(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f52de7a",
   "metadata": {},
   "source": [
    "### klue/bert-base num_layersX2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d5dc7948",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.tokenizer_type = 'pretrained' \n",
    "config.pretrained_model_name = \"klue/bert-base\"\n",
    "config.num_layers = 8\n",
    "config.d_model = 256\n",
    "config.ff_dim = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1006cb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사전학습된 토크나이저 로드 중: klue/bert-base\n",
      "[Config Finalized] Vocab: 32000\n",
      " >> IDs - PAD:0, UNK:1, CLS:2, SEP:3, MASK:4\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(config, df[\"conversation\"].tolist())\n",
    "config.update_from_tokenizer(tokenizer) # Vocab Size 동기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bb593b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df[\"label\"])\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    ClassificationDataset(train_df[\"conversation\"].tolist(), train_df[\"label\"].tolist(), tokenizer, config, is_train=True),\n",
    "    batch_size=config.batch_size, shuffle=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    ClassificationDataset(val_df[\"conversation\"].tolist(), val_df[\"label\"].tolist(), tokenizer, config, is_train=False),\n",
    "    batch_size=config.batch_size\n",
    ")\n",
    "# debug(train_loader, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "05f24419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): StandardBertModel(\n",
       "    (token_emb): Embedding(32000, 256)\n",
       "    (pos_emb): Embedding(128, 256)\n",
       "    (seg_emb): Embedding(2, 256)\n",
       "    (norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-7): 8 x ModuleDict(\n",
       "        (mha): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.3, inplace=False)\n",
       "          (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (4): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (pooler): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (1): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (classifier): Linear(in_features=256, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. 모델 초기화\n",
    "backbone = StandardBertModel(config)\n",
    "model = BertForSequenceClassification(backbone, config, num_labels=5).to(device)\n",
    "model.apply(init_weights) # 가중치 안정화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9d9c6dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  [Train] Loss: 1.6102 | Acc: 0.1994\n",
      "  [Val]   Loss: 1.6093 | Acc: 0.2000 | F1: 0.0667\n",
      "  >> Best F1 Updated! (0.0667)\n",
      "  >> Best Loss Updated! (1.6093)\n",
      "  >> Best model Updated! (0.0667, 1.6093)\n",
      "--------------------------------------------------\n",
      "Epoch 2/100\n",
      "  [Train] Loss: 1.6098 | Acc: 0.2008\n",
      "  [Val]   Loss: 1.6075 | Acc: 0.2146 | F1: 0.1070\n",
      "  >> Best F1 Updated! (0.1070)\n",
      "  >> Best Loss Updated! (1.6075)\n",
      "  >> Best model Updated! (0.1070, 1.6075)\n",
      "--------------------------------------------------\n",
      "Epoch 3/100\n",
      "  [Train] Loss: 1.5905 | Acc: 0.2559\n",
      "  [Val]   Loss: 1.4509 | Acc: 0.4112 | F1: 0.3076\n",
      "  >> Best F1 Updated! (0.3076)\n",
      "  >> Best Loss Updated! (1.4509)\n",
      "  >> Best model Updated! (0.3076, 1.4509)\n",
      "--------------------------------------------------\n",
      "Epoch 4/100\n",
      "  [Train] Loss: 1.3683 | Acc: 0.4160\n",
      "  [Val]   Loss: 1.3076 | Acc: 0.5281 | F1: 0.4728\n",
      "  >> Best F1 Updated! (0.4728)\n",
      "  >> Best Loss Updated! (1.3076)\n",
      "  >> Best model Updated! (0.4728, 1.3076)\n",
      "--------------------------------------------------\n",
      "Epoch 5/100\n",
      "  [Train] Loss: 1.2335 | Acc: 0.5292\n",
      "  [Val]   Loss: 1.1476 | Acc: 0.6348 | F1: 0.5964\n",
      "  >> Best F1 Updated! (0.5964)\n",
      "  >> Best Loss Updated! (1.1476)\n",
      "  >> Best model Updated! (0.5964, 1.1476)\n",
      "--------------------------------------------------\n",
      "Epoch 6/100\n",
      "  [Train] Loss: 1.0596 | Acc: 0.6531\n",
      "  [Val]   Loss: 1.0335 | Acc: 0.6798 | F1: 0.6536\n",
      "  >> Best F1 Updated! (0.6536)\n",
      "  >> Best Loss Updated! (1.0335)\n",
      "  >> Best model Updated! (0.6536, 1.0335)\n",
      "--------------------------------------------------\n",
      "Epoch 7/100\n",
      "  [Train] Loss: 0.9328 | Acc: 0.7559\n",
      "  [Val]   Loss: 0.9596 | Acc: 0.7382 | F1: 0.7268\n",
      "  >> Best F1 Updated! (0.7268)\n",
      "  >> Best Loss Updated! (0.9596)\n",
      "  >> Best model Updated! (0.7268, 0.9596)\n",
      "--------------------------------------------------\n",
      "Epoch 8/100\n",
      "  [Train] Loss: 0.8029 | Acc: 0.8393\n",
      "  [Val]   Loss: 0.8922 | Acc: 0.7528 | F1: 0.7510\n",
      "  >> Best F1 Updated! (0.7510)\n",
      "  >> Best Loss Updated! (0.8922)\n",
      "  >> Best model Updated! (0.7510, 0.8922)\n",
      "--------------------------------------------------\n",
      "Epoch 9/100\n",
      "  [Train] Loss: 0.6962 | Acc: 0.8837\n",
      "  [Val]   Loss: 0.8549 | Acc: 0.7933 | F1: 0.7968\n",
      "  >> Best F1 Updated! (0.7968)\n",
      "  >> Best Loss Updated! (0.8549)\n",
      "  >> Best model Updated! (0.7968, 0.8549)\n",
      "--------------------------------------------------\n",
      "Epoch 10/100\n",
      "  [Train] Loss: 0.6212 | Acc: 0.9157\n",
      "  [Val]   Loss: 0.8117 | Acc: 0.8292 | F1: 0.8298\n",
      "  >> Best F1 Updated! (0.8298)\n",
      "  >> Best Loss Updated! (0.8117)\n",
      "  >> Best model Updated! (0.8298, 0.8117)\n",
      "--------------------------------------------------\n",
      "Epoch 11/100\n",
      "  [Train] Loss: 0.5764 | Acc: 0.9284\n",
      "  [Val]   Loss: 0.8281 | Acc: 0.8124 | F1: 0.8134\n",
      "  >> EarlyStopping counter: 1 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 12/100\n",
      "  [Train] Loss: 0.5383 | Acc: 0.9379\n",
      "  [Val]   Loss: 0.8555 | Acc: 0.8034 | F1: 0.8020\n",
      "  >> EarlyStopping counter: 2 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 13/100\n",
      "  [Train] Loss: 0.5075 | Acc: 0.9494\n",
      "  [Val]   Loss: 0.8716 | Acc: 0.8056 | F1: 0.8054\n",
      "  >> EarlyStopping counter: 3 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 14/100\n",
      "  [Train] Loss: 0.4911 | Acc: 0.9559\n",
      "  [Val]   Loss: 0.9136 | Acc: 0.8101 | F1: 0.8122\n",
      "  >> EarlyStopping counter: 4 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 15/100\n",
      "  [Train] Loss: 0.4577 | Acc: 0.9699\n",
      "  [Val]   Loss: 0.8976 | Acc: 0.8124 | F1: 0.8140\n",
      "  >> EarlyStopping counter: 5 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 16/100\n",
      "  [Train] Loss: 0.4593 | Acc: 0.9697\n",
      "  [Val]   Loss: 0.9350 | Acc: 0.8112 | F1: 0.8110\n",
      "  >> EarlyStopping counter: 6 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 17/100\n",
      "  [Train] Loss: 0.4540 | Acc: 0.9694\n",
      "  [Val]   Loss: 0.9080 | Acc: 0.8112 | F1: 0.8139\n",
      "  >> EarlyStopping counter: 7 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 18/100\n",
      "  [Train] Loss: 0.4395 | Acc: 0.9792\n",
      "  [Val]   Loss: 0.9322 | Acc: 0.8180 | F1: 0.8202\n",
      "  >> EarlyStopping counter: 8 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 19/100\n",
      "  [Train] Loss: 0.4413 | Acc: 0.9792\n",
      "  [Val]   Loss: 0.9904 | Acc: 0.8011 | F1: 0.8033\n",
      "  >> EarlyStopping counter: 9 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 20/100\n",
      "  [Train] Loss: 0.4470 | Acc: 0.9772\n",
      "  [Val]   Loss: 0.9799 | Acc: 0.7989 | F1: 0.8046\n",
      "  >> EarlyStopping counter: 10 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 21/100\n",
      "  [Train] Loss: 0.4284 | Acc: 0.9846\n",
      "  [Val]   Loss: 0.9297 | Acc: 0.8157 | F1: 0.8181\n",
      "  >> EarlyStopping counter: 11 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 22/100\n",
      "  [Train] Loss: 0.4275 | Acc: 0.9860\n",
      "  [Val]   Loss: 1.0046 | Acc: 0.7910 | F1: 0.7965\n",
      "  >> EarlyStopping counter: 12 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 23/100\n",
      "  [Train] Loss: 0.4320 | Acc: 0.9837\n",
      "  [Val]   Loss: 0.9391 | Acc: 0.8191 | F1: 0.8206\n",
      "  >> EarlyStopping counter: 13 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 24/100\n",
      "  [Train] Loss: 0.4277 | Acc: 0.9848\n",
      "  [Val]   Loss: 0.9777 | Acc: 0.8056 | F1: 0.8067\n",
      "  >> EarlyStopping counter: 14 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 25/100\n",
      "  [Train] Loss: 0.4202 | Acc: 0.9879\n",
      "  [Val]   Loss: 0.9641 | Acc: 0.8112 | F1: 0.8145\n",
      "  >> EarlyStopping counter: 15 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 26/100\n",
      "  [Train] Loss: 0.4134 | Acc: 0.9916\n",
      "  [Val]   Loss: 0.9751 | Acc: 0.8124 | F1: 0.8145\n",
      "  >> EarlyStopping counter: 16 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 27/100\n",
      "  [Train] Loss: 0.4175 | Acc: 0.9896\n",
      "  [Val]   Loss: 0.9963 | Acc: 0.8045 | F1: 0.8070\n",
      "  >> EarlyStopping counter: 17 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 28/100\n",
      "  [Train] Loss: 0.4192 | Acc: 0.9885\n",
      "  [Val]   Loss: 0.9431 | Acc: 0.8236 | F1: 0.8242\n",
      "  >> EarlyStopping counter: 18 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 29/100\n",
      "  [Train] Loss: 0.4163 | Acc: 0.9893\n",
      "  [Val]   Loss: 0.9554 | Acc: 0.8213 | F1: 0.8221\n",
      "  >> EarlyStopping counter: 19 out of 20\n",
      "--------------------------------------------------\n",
      "Epoch 30/100\n",
      "  [Train] Loss: 0.4160 | Acc: 0.9893\n",
      "  [Val]   Loss: 0.9914 | Acc: 0.8034 | F1: 0.8074\n",
      "  >> EarlyStopping counter: 20 out of 20\n",
      "성능 개선이 없어 30 에포크에서 학습을 조기 종료합니다.\n",
      "학습이 완료되었습니다. 최종 지표 분석을 시작합니다.\n",
      ">> 최고 성능 모델 복원 완료 (Epoch 10)\n",
      "val_acc:0.8292134831460675, val_acc: 0.8292134831460675, f1:0.8297864458319302\n",
      ">> Confusion Matrix image saved to ./results/klue_bert-base_d256_f1024_l8/final_best_confusion_matrix.png\n",
      ">> [Success] 시각화 결과가 저장되었습니다: ./results/klue_bert-base\n",
      ">> 모든 최종 결과물이 ./results/klue_bert-base 에 저장되었습니다.\n",
      "CPU times: user 4min 21s, sys: 22.8 s, total: 4min 44s\n",
      "Wall time: 4min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = train(\n",
    "   config, model, train_loader, val_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3090c48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 가중치 로드 완료 (Epoch: 9, F1: 0.8298)\n",
      "모델 가중치 로드 완료 (Epoch: 9, F1: 0.8298)\n",
      "사전학습된 토크나이저 로드 중: klue/bert-base\n",
      "추론 시작... (테스트 데이터: 500개)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 16/16 [00:00<00:00, 43.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최종 결과 저장 완료: ./dataset/klue_bert-base_d256_f1024_l8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>class</th>\n",
       "      <th>text_label</th>\n",
       "      <th>confidence</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>conversation</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t_000</td>\n",
       "      <td>1</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>0.7285</td>\n",
       "      <td>아가씨 담배 ##한 ##갑 ##주 ##소 네 원 ##입니다 어 네 지갑 ##어 ##...</td>\n",
       "      <td>아가씨 담배한갑주소 네 4500원입니다 어 네 지갑어디갔지 에이 버스에서 잃어버렸나...</td>\n",
       "      <td>아가씨 담배한갑주소 네 원입니다 어 네 지갑어디갔지 에이 버스에서 잃어버렸나보네 그...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t_001</td>\n",
       "      <td>2</td>\n",
       "      <td>직장 내 괴롭힘 대화</td>\n",
       "      <td>0.8577</td>\n",
       "      <td>우리 ##팀 ##에서 다른 ##팀 ##으로 갈 사람 없 ##나 그럼 영지 ##씨 #...</td>\n",
       "      <td>우리팀에서 다른팀으로 갈 사람 없나? 그럼 영지씨가 가는건 어때?&nbsp;&nbsp;네? 제가요? ...</td>\n",
       "      <td>우리팀에서 다른팀으로 갈 사람 없나 그럼 영지씨가 가는건 어때 네 제가요 그렇지 달...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t_002</td>\n",
       "      <td>2</td>\n",
       "      <td>직장 내 괴롭힘 대화</td>\n",
       "      <td>0.5399</td>\n",
       "      <td>너 오늘 그게 뭐 ##야 네 제 ##가 뭘 잘못 ##했 ##나 ##요 제대로 좀 하...</td>\n",
       "      <td>너 오늘 그게 뭐야 네 제가 뭘 잘못했나요.? 제대로 좀 하지 네 똑바로 좀 하지 ...</td>\n",
       "      <td>너 오늘 그게 뭐야 네 제가 뭘 잘못했나요 제대로 좀 하지 네 똑바로 좀 하지 행실...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t_003</td>\n",
       "      <td>4</td>\n",
       "      <td>일반 대화</td>\n",
       "      <td>0.7051</td>\n",
       "      <td>이거 들어 ##바 와 이 노래 진짜 좋 ##다 그치 요즘 이 것 ##만 들어 진짜 ...</td>\n",
       "      <td>이거 들어바 와 이 노래 진짜 좋다 그치 요즘 이 것만 들어 진짜 너무 좋다 내가 ...</td>\n",
       "      <td>이거 들어바 와 이 노래 진짜 좋다 그치 요즘 이 것만 들어 진짜 너무 좋다 내가 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t_004</td>\n",
       "      <td>3</td>\n",
       "      <td>기타 괴롭힘 대화</td>\n",
       "      <td>0.8351</td>\n",
       "      <td>아무튼 앞 ##으로 니 ##가 내 와이파이 ##야 응 와이파이 온 켰 ##어 반말 ...</td>\n",
       "      <td>아무튼 앞으로 니가 내 와이파이야. .응 와이파이 온. 켰어. 반말? 주인님이라고도...</td>\n",
       "      <td>아무튼 앞으로 니가 내 와이파이야 응 와이파이 온 켰어 반말 주인님이라고도 말해야지...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>t_495</td>\n",
       "      <td>2</td>\n",
       "      <td>직장 내 괴롭힘 대화</td>\n",
       "      <td>0.8543</td>\n",
       "      <td>미나 ##씨 휴가 결제 올리 ##기 전 ##에 저 ##랑 상의 ##하라 ##고 말 ...</td>\n",
       "      <td>미나씨 휴가 결제 올리기 전에 저랑 상의하라고 말한거 기억해요? 네 합니다. 보고서...</td>\n",
       "      <td>미나씨 휴가 결제 올리기 전에 저랑 상의하라고 말한거 기억해요 네 합니다 보고서를 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>t_496</td>\n",
       "      <td>0</td>\n",
       "      <td>협박 대화</td>\n",
       "      <td>0.5493</td>\n",
       "      <td>교수 ##님 제 논문 ##에 제 이름 ##이 없 ##나 ##요 아 무슨 논문 ##말...</td>\n",
       "      <td>교수님 제 논문에 제 이름이 없나요?&nbsp;&nbsp;아 무슨 논문말이야?&nbsp;&nbsp;지난 번 냈던 논문이...</td>\n",
       "      <td>교수님 제 논문에 제 이름이 없나요 아 무슨 논문말이야 지난 번 냈던 논문이요 그거...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>t_497</td>\n",
       "      <td>1</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>0.8242</td>\n",
       "      <td>야 너 네 저 ##요 그래 너 왜 ##요 돈 ##좀 줘 ##봐 돈 없 ##어요 돈 ...</td>\n",
       "      <td>야 너&nbsp;&nbsp;네 저요? 그래 너 왜요 돈좀 줘봐&nbsp;&nbsp;돈 없어요 돈이 왜 없어 지갑은 폼이...</td>\n",
       "      <td>야 너 네 저요 그래 너 왜요 돈좀 줘봐 돈 없어요 돈이 왜 없어 지갑은 폼이니 진...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>t_498</td>\n",
       "      <td>2</td>\n",
       "      <td>직장 내 괴롭힘 대화</td>\n",
       "      <td>0.5860</td>\n",
       "      <td>야 너 빨리 안 뛰어 ##와 너 이 환자 제대로 봤 ##어 안 봤 ##어 어제 저녁...</td>\n",
       "      <td>야 너 빨리 안 뛰어와? 너 이 환자 제대로 봤어 안 봤어 어제 저녁부터 계속 보다...</td>\n",
       "      <td>야 너 빨리 안 뛰어와 너 이 환자 제대로 봤어 안 봤어 어제 저녁부터 계속 보다가...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>t_499</td>\n",
       "      <td>0</td>\n",
       "      <td>협박 대화</td>\n",
       "      <td>0.6202</td>\n",
       "      <td>엄마 저 그 돈 안 ##해 ##주 ##시 ##면 정말 큰일 ##나 ##요 이유 ##...</td>\n",
       "      <td>엄마 저 그 돈 안해주시면 정말 큰일나요.&nbsp;&nbsp;이유도 말하지 않고. 몇번째니 경민아....</td>\n",
       "      <td>엄마 저 그 돈 안해주시면 정말 큰일나요 이유도 말하지 않고 몇번째니 경민아 엄마 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       idx  class   text_label  confidence  \\\n",
       "0    t_000      1        갈취 대화      0.7285   \n",
       "1    t_001      2  직장 내 괴롭힘 대화      0.8577   \n",
       "2    t_002      2  직장 내 괴롭힘 대화      0.5399   \n",
       "3    t_003      4        일반 대화      0.7051   \n",
       "4    t_004      3    기타 괴롭힘 대화      0.8351   \n",
       "..     ...    ...          ...         ...   \n",
       "495  t_495      2  직장 내 괴롭힘 대화      0.8543   \n",
       "496  t_496      0        협박 대화      0.5493   \n",
       "497  t_497      1        갈취 대화      0.8242   \n",
       "498  t_498      2  직장 내 괴롭힘 대화      0.5860   \n",
       "499  t_499      0        협박 대화      0.6202   \n",
       "\n",
       "                                        tokenized_text  \\\n",
       "0    아가씨 담배 ##한 ##갑 ##주 ##소 네 원 ##입니다 어 네 지갑 ##어 ##...   \n",
       "1    우리 ##팀 ##에서 다른 ##팀 ##으로 갈 사람 없 ##나 그럼 영지 ##씨 #...   \n",
       "2    너 오늘 그게 뭐 ##야 네 제 ##가 뭘 잘못 ##했 ##나 ##요 제대로 좀 하...   \n",
       "3    이거 들어 ##바 와 이 노래 진짜 좋 ##다 그치 요즘 이 것 ##만 들어 진짜 ...   \n",
       "4    아무튼 앞 ##으로 니 ##가 내 와이파이 ##야 응 와이파이 온 켰 ##어 반말 ...   \n",
       "..                                                 ...   \n",
       "495  미나 ##씨 휴가 결제 올리 ##기 전 ##에 저 ##랑 상의 ##하라 ##고 말 ...   \n",
       "496  교수 ##님 제 논문 ##에 제 이름 ##이 없 ##나 ##요 아 무슨 논문 ##말...   \n",
       "497  야 너 네 저 ##요 그래 너 왜 ##요 돈 ##좀 줘 ##봐 돈 없 ##어요 돈 ...   \n",
       "498  야 너 빨리 안 뛰어 ##와 너 이 환자 제대로 봤 ##어 안 봤 ##어 어제 저녁...   \n",
       "499  엄마 저 그 돈 안 ##해 ##주 ##시 ##면 정말 큰일 ##나 ##요 이유 ##...   \n",
       "\n",
       "                                          conversation  \\\n",
       "0    아가씨 담배한갑주소 네 4500원입니다 어 네 지갑어디갔지 에이 버스에서 잃어버렸나...   \n",
       "1    우리팀에서 다른팀으로 갈 사람 없나? 그럼 영지씨가 가는건 어때?  네? 제가요? ...   \n",
       "2    너 오늘 그게 뭐야 네 제가 뭘 잘못했나요.? 제대로 좀 하지 네 똑바로 좀 하지 ...   \n",
       "3    이거 들어바 와 이 노래 진짜 좋다 그치 요즘 이 것만 들어 진짜 너무 좋다 내가 ...   \n",
       "4    아무튼 앞으로 니가 내 와이파이야. .응 와이파이 온. 켰어. 반말? 주인님이라고도...   \n",
       "..                                                 ...   \n",
       "495  미나씨 휴가 결제 올리기 전에 저랑 상의하라고 말한거 기억해요? 네 합니다. 보고서...   \n",
       "496  교수님 제 논문에 제 이름이 없나요?  아 무슨 논문말이야?  지난 번 냈던 논문이...   \n",
       "497  야 너  네 저요? 그래 너 왜요 돈좀 줘봐  돈 없어요 돈이 왜 없어 지갑은 폼이...   \n",
       "498  야 너 빨리 안 뛰어와? 너 이 환자 제대로 봤어 안 봤어 어제 저녁부터 계속 보다...   \n",
       "499  엄마 저 그 돈 안해주시면 정말 큰일나요.  이유도 말하지 않고. 몇번째니 경민아....   \n",
       "\n",
       "                                          cleaned_text  \n",
       "0    아가씨 담배한갑주소 네 원입니다 어 네 지갑어디갔지 에이 버스에서 잃어버렸나보네 그...  \n",
       "1    우리팀에서 다른팀으로 갈 사람 없나 그럼 영지씨가 가는건 어때 네 제가요 그렇지 달...  \n",
       "2    너 오늘 그게 뭐야 네 제가 뭘 잘못했나요 제대로 좀 하지 네 똑바로 좀 하지 행실...  \n",
       "3    이거 들어바 와 이 노래 진짜 좋다 그치 요즘 이 것만 들어 진짜 너무 좋다 내가 ...  \n",
       "4    아무튼 앞으로 니가 내 와이파이야 응 와이파이 온 켰어 반말 주인님이라고도 말해야지...  \n",
       "..                                                 ...  \n",
       "495  미나씨 휴가 결제 올리기 전에 저랑 상의하라고 말한거 기억해요 네 합니다 보고서를 ...  \n",
       "496  교수님 제 논문에 제 이름이 없나요 아 무슨 논문말이야 지난 번 냈던 논문이요 그거...  \n",
       "497  야 너 네 저요 그래 너 왜요 돈좀 줘봐 돈 없어요 돈이 왜 없어 지갑은 폼이니 진...  \n",
       "498  야 너 빨리 안 뛰어와 너 이 환자 제대로 봤어 안 봤어 어제 저녁부터 계속 보다가...  \n",
       "499  엄마 저 그 돈 안해주시면 정말 큰일나요 이유도 말하지 않고 몇번째니 경민아 엄마 ...  \n",
       "\n",
       "[500 rows x 7 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_and_predict(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
